<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository><bug fixdate="2015-03-16 07:03:21" id="11714" opendate="2015-03-13 08:06:19"><buginformation><summary>[HADOOP-11714] Add more trace log4j messages to SpanReceiverHost - ASF JIRA</summary><description>Add more trace log4j messages to SpanReceiverHost</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.SpanReceiverHost.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.SpanReceiverHost.java</file></fixedFiles></bug><bug fixdate="2015-03-02 09:15:54" id="11658" opendate="2015-03-02 05:47:26"><buginformation><summary>[HADOOP-11658] Externalize io.compression.codecs property - ASF JIRA</summary><description>A minor code refactoring, externalizing io.compression.codecs as configuration key.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CompressionCodecFactory.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.CommonConfigurationKeys.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.compress.TestCodecFactory.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CompressionCodecFactory.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.CommonConfigurationKeys.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.compress.TestCodecFactory.java</file></fixedFiles></bug><bug fixdate="2015-03-05 07:10:38" id="11648" opendate="2015-02-28 08:38:34"><buginformation><summary>[HADOOP-11648] Set DomainSocketWatcher thread name explicitly - ASF JIRA</summary><description>while working at HADOOP-11604, seems the current DomainSocketWatcher thread name is not set explicitly, e.g. in our cluster, the format is like: Thread-25, Thread-303670 or sth else. Here Thread-25 seems came from Datanode.initDataXceiver, and once this thread die, the Xceiver leak will be found. I think it'd better to set the thread name, so we can debug issue easier in further.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.net.unix.TestDomainSocketWatcher.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocketWatcher.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.java</file></fixedFiles></bug><bug fixdate="2015-03-06 11:45:15" id="11642" opendate="2015-02-27 05:05:44"><buginformation><summary>[HADOOP-11642] Upgrade azure sdk version from 0.6.0 to 2.0.0 - ASF JIRA</summary><description>hadoop-azure uses unsupported version of azure sdk (0.6.0).Upgrade it to 2.0.0 Breaking changes :https://github.com/Azure/azure-storage-java/blob/master/BreakingChanges.txt</description></buginformation><fixedFiles><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfRenewingLease.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfRenewingLease.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ErrorMetricUpdater.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobOutputStream.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobFormatHelpers.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfRenewingLease.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ErrorMetricUpdater.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobFormatHelpers.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterface.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ErrorMetricUpdater.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestBlobDataValidation.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.MockStorageInterface.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestContainerChecks.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SendRequestIntercept.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobOutputStream.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SendRequestIntercept.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestBlobDataValidation.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobFormatHelpers.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobInputStream.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestBlobDataValidation.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestContainerChecks.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobInputStream.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SendRequestIntercept.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestContainerChecks.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfThrottlingIntercept.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterface.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobInputStream.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterface.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.MockStorageInterface.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.MockStorageInterface.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfThrottlingIntercept.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobOutputStream.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.SelfThrottlingIntercept.java</file></fixedFiles></bug><bug fixdate="2015-02-25 07:27:25" id="11632" opendate="2015-02-25 12:11:39"><buginformation><summary>[HADOOP-11632] Cleanup Find.java to remove SupressWarnings annotations - ASF JIRA</summary><description>There are some SuppressWarnings annotations in Find.java. We should fix them.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.find.Find.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.find.Find.java</file></fixedFiles></bug><bug fixdate="2015-02-26 05:18:46" id="11620" opendate="2015-02-21 08:36:41"><buginformation><summary>[HADOOP-11620] Add support for load balancing across a group of KMS for HA - ASF JIRA</summary><description>This patch needs to add support for : specification of multiple hostnames in the kms key provider uri KMS client to load balance requests across the hosts specified in the kms keyprovider uri.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-kms.src.test.java.org.apache.hadoop.crypto.key.kms.server.TestKMS.java</file><file>a.hadoop-common-project.hadoop-kms.src.test.java.org.apache.hadoop.crypto.key.kms.server.TestKMS.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.crypto.key.TestKeyProviderFactory.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.java</file></fixedFiles></bug><bug fixdate="2015-02-20 09:59:09" id="11607" opendate="2015-02-17 09:30:07"><buginformation><summary>[HADOOP-11607] Reduce log spew in S3AFileSystem - ASF JIRA</summary><description>S3AFileSystem generates INFO level logs in open and rename, which are not necessary.</description></buginformation><fixedFiles><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file></fixedFiles></bug><bug fixdate="2015-02-18 09:02:17" id="11599" opendate="2015-02-16 06:19:57"><buginformation><summary>[HADOOP-11599] Client#getTimeout should use IPC_CLIENT_PING_DEFAULT when IPC_CLIENT_PING_KEY is not configured. - ASF JIRA</summary><description>Client#getTimeout should use IPC_CLIENT_PING_DEFAULT instead of hard-coded value (true) when IPC_CLIENT_PING_KEY is not configured. &#13;
    if (!conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true)) {</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ipc.TestIPC.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ipc.TestIPC.java</file></fixedFiles></bug><bug fixdate="2015-02-14 03:14:11" id="11589" opendate="2015-02-12 08:27:39"><buginformation><summary>[HADOOP-11589] NetUtils.createSocketAddr should trim the input URI - ASF JIRA</summary><description>NetUtils.createSocketAddr does not trim the input URI, should be trimmed. HDFS-7684 and HADOOP-9869 are trying to trim some URIs to be passed to the method, however, not all of the inputs have been trimmed already.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.conf.TestConfiguration.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.net.TestNetUtils.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.conf.TestConfiguration.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.net.TestNetUtils.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.java</file></fixedFiles></bug><bug fixdate="2015-02-12 10:44:34" id="11586" opendate="2015-02-11 11:01:17"><buginformation><summary>[HADOOP-11586] Update use of Iterator to Iterable in AbstractMetricsContext.java - ASF JIRA</summary><description>Found these using the IntelliJ Findbugs-IDEA plugin, which uses findbugs3.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics.spi.AbstractMetricsContext.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics.spi.AbstractMetricsContext.java</file></fixedFiles></bug><bug fixdate="2015-02-27 08:48:35" id="11569" opendate="2015-02-10 06:51:57"><buginformation><summary>[HADOOP-11569] Provide Merge API for MapFile to merge multiple similar MapFiles to one MapFile - ASF JIRA</summary><description>If there are multiple similar MapFiles of the same keyClass and value classes, then these can be merged together to One MapFile to allow search easier. Provide an API similar to SequenceFile#merge(). Merging will be easy with the fact that MapFiles are already sorted.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.MapFile.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.MapFile.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestMapFile.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestMapFile.java</file></fixedFiles></bug><bug fixdate="2015-02-04 12:22:06" id="11544" opendate="2015-02-04 02:53:38"><buginformation><summary>[HADOOP-11544] Remove unused configuration keys for tracing - ASF JIRA</summary><description>CommonConfigurationKeys.HADOOP_TRACE_SAMPLER* are no longer used.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.CommonConfigurationKeys.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.CommonConfigurationKeys.java</file></fixedFiles></bug><bug fixdate="2015-02-06 09:02:37" id="11520" opendate="2015-01-29 03:50:03"><buginformation><summary>[HADOOP-11520] Clean incomplete multi-part uploads in S3A tests - ASF JIRA</summary><description>As proposed in HADOOP-11488. This patch activates the purging functionality of s3a at the start of each test. This cleans up any in-progress multi-part uploads in the test bucket, preventing unknowing users from eternally paying Amazon for the space of the already uploaded parts of previous tests that failed during a multi-part upload. People who have run the s3a tests should run a single test (evidently after this patch is applied) against all their testbuckets (or manually abort multipart).</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.test.java.org.apache.hadoop.fs.s3a.S3ATestUtils.java</file><file>hadoop-tools.hadoop-aws.src.test.java.org.apache.hadoop.fs.s3a.S3ATestUtils.java</file></fixedFiles></bug><bug fixdate="2015-02-06 10:02:45" id="11506" opendate="2015-01-22 10:40:17"><buginformation><summary>[HADOOP-11506] Configuration variable expansion regex expensive for long values - ASF JIRA</summary><description>Profiling several large Hadoop jobs, we discovered that a surprising amount of time was spent inside Configuration.get, more specifically, in regex matching caused by the substituteVars call.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.conf.TestConfiguration.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.conf.TestConfiguration.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.java</file></fixedFiles></bug><bug fixdate="2015-01-31 12:03:33" id="11498" opendate="2015-01-21 02:13:33"><buginformation><summary>[HADOOP-11498] Bump the version of HTrace to 3.1.0-incubating - ASF JIRA</summary><description>The package is renamed from org.htrace to org.apache.htrace.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.TraceUtils.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTracingShortCircuitLocalRead.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.CachePoolIterator.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInputStream.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTracingShortCircuitLocalRead.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Sender.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInotifyEventInputStream.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.WritableRpcEngine.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockStorageLocationUtil.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader2.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.EncryptionZoneIterator.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockReaderLocal.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.RemoteBlockReader.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.EncryptionZoneIterator.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.SpanReceiverHost.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTracing.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTraceAdmin.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.ProtobufRpcEngine.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.CachePoolIterator.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.tracing.TestTraceUtils.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProtoUtil.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTraceAdmin.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.SpanReceiverHost.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockReaderLocal.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSInotifyEventInputStream.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.tracing.TestTracing.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.TraceSamplerFactory.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Sender.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.tracing.TraceSamplerFactory.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ProtoUtil.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockStorageLocationUtil.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockReaderLocalLegacy.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockReaderLocalLegacy.java</file></fixedFiles></bug><bug fixdate="2015-01-15 12:04:53" id="11483" opendate="2014-12-19 09:30:16"><buginformation><summary>[HADOOP-11483] HardLink.java should use the jdk7 createLink method - ASF JIRA</summary><description>Now that we are using jdk7, HardLink.java should use the jdk7 createLink method rather than our shell commands or JNI methods. Note that we cannot remove all of the JNI / shell commands unless we remove the code which is checking the link count, something that jdk7 doesn't provide (at least, I don't think it does)</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.TestHardLink.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.HardLink.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.TestHardLink.java</file></fixedFiles></bug><bug fixdate="2015-01-15 02:06:31" id="11481" opendate="2014-09-16 03:35:02"><buginformation><summary>[HADOOP-11481] ClassCastException while using a key created by keytool to create encryption zone. - ASF JIRA</summary><description>I'm using transparent encryption. If I create a key for KMS keystore via keytool and use the key to create an encryption zone. I get a ClassCastException rather than an exception with decent error message. I know we should use 'hadoop key create' to create a key. It's better to provide an decent error message to remind user to use the right way to create a KMS key. [LOG] ERROR[user=hdfs] Method:'GET' Exception:'java.lang.ClassCastException: javax.crypto.spec.SecretKeySpec cannot be cast to org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata'</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.crypto.key.TestKeyProviderFactory.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.JavaKeyStoreProvider.java</file></fixedFiles></bug><bug fixdate="2015-01-05 07:50:17" id="11455" opendate="2014-12-30 09:26:55"><buginformation><summary>[HADOOP-11455] KMS and Credential CLI should request confirmation for deletion by default - ASF JIRA</summary><description>The hadoop key delete and hadoop credential delete currently only ask for confirmation of the delete if -i is specified. Asking for confirmation should be the default action for both.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.crypto.key.TestKeyShell.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.alias.TestCredShell.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.crypto.key.TestKeyShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.alias.TestCredShell.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file></fixedFiles></bug><bug fixdate="2014-12-26 06:20:56" id="11448" opendate="2014-12-24 04:31:24"><buginformation><summary>[HADOOP-11448] Fix findbugs warnings in FileBasedIPList - ASF JIRA</summary><description>Now there are 3 findbugs warnings in hadoop-common package. https://builds.apache.org/job/PreCommit-HADOOP-Build/5336//artifact/patchprocess/newPatchFindbugsWarningshadoop-common.html &#13;
Bug type RV_RETURN_VALUE_IGNORED At ActiveStandbyElector.java:[line 1067]&#13;
Bug type AT_OPERATION_SEQUENCE_ON_CONCURRENT_ABSTRACTION At NetUtils.java:[line 291]&#13;
 The above two warnings will be fixed by HADOOP-11433. This issue is to fix the last one. &#13;
Bug type DLS_DEAD_LOCAL_STORE At FileBasedIPList.java:[line 53]</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.FileBasedIPList.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.FileBasedIPList.java</file></fixedFiles></bug><bug fixdate="2015-01-29 09:14:02" id="11441" opendate="2014-12-22 09:54:38"><buginformation><summary>[HADOOP-11441] Hadoop-azure: Change few methods scope to public - ASF JIRA</summary><description>TestWindowsAzureTableSinkSetup class test cases have dependencies with hadoop-azure classes, however few functions in hadoop azure classes are having default access and are not visible outside package. AzureBlobStorageTestAccount.createTestAccount() AzureNativeFileSystemStore.getAccountKeyFromConfiguration()</description></buginformation><fixedFiles><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file></fixedFiles></bug><bug fixdate="2015-02-19 01:57:41" id="11440" opendate="2014-12-22 07:31:00"><buginformation><summary>[HADOOP-11440] Use "test.build.data" instead of "build.test.dir" for testing in ClientBaseWithFixes - ASF JIRA</summary><description>In ClientBaseWithFixes.java, the base directory for tests are set in the following: &#13;
    static final File BASETEST =&#13;
        new File(System.getProperty("build.test.dir", "build"));&#13;
 There is no property "build.test.dir", so BASETEST is always "build". We should use "test.build.data" instead of "build.test.dir".</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ha.ClientBaseWithFixes.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ha.ClientBaseWithFixes.java</file></fixedFiles></bug><bug fixdate="2014-12-19 03:42:31" id="11430" opendate="2014-12-18 10:46:36"><buginformation><summary>[HADOOP-11430] Add GenericTestUtils#disableLog, GenericTestUtils#setLogLevel - ASF JIRA</summary><description>Now that we are using both commons-logging and slf4j, we can no longer rely on just casting the Log object to a Log4JLogger and calling setLevel on that. With org.slf4j.Logger objects, we need to look up the underlying Log4JLogger using LogManager#getLogger. This patch adds GenericTestUtils#disableLog and GenericTestUtils#setLogLevel functions which hide this complexity from unit tests, just allowing the tests to call disableLog or setLogLevel, and have GenericTestUtils figure out the right thing to do based on the log / logger type.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.DFSTestUtil.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.TestDFSShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.FileContextCreateMkdirBaseTest.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.TestDFSShell.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.DFSTestUtil.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.test.GenericTestUtils.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.FileContextCreateMkdirBaseTest.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.test.GenericTestUtils.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper.java</file></fixedFiles></bug><bug fixdate="2014-12-18 07:05:48" id="11427" opendate="2014-12-18 12:37:01"><buginformation><summary>[HADOOP-11427] ChunkedArrayList: fix removal via iterator and implement get - ASF JIRA</summary><description>ChunkedArrayList: implement removal via iterator and get. Previously, calling remove on a ChunkedArrayList iterator would cause the returned size to be incorrect later.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.util.TestChunkedArrayList.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.util.TestChunkedArrayList.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ChunkedArrayList.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ChunkedArrayList.java</file></fixedFiles></bug><bug fixdate="2014-12-19 09:13:38" id="11422" opendate="2014-12-18 02:35:45"><buginformation><summary>[HADOOP-11422] Check CryptoCodec is AES-CTR for Crypto input/output stream - ASF JIRA</summary><description>CryptoInputStream and CryptoOutputStream require AES-CTR as the algorithm/mode, although there is only AES-CTR implementation currently, but we'd better to check it.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoStreamUtils.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoInputStream.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoInputStream.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoStreamUtils.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoOutputStream.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.UnsupportedCodecException.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoOutputStream.java</file></fixedFiles></bug><bug fixdate="2014-12-17 11:18:56" id="11421" opendate="2014-12-17 07:06:07"><buginformation><summary>[HADOOP-11421] Add IOUtils#listDirectory - ASF JIRA</summary><description>We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestIOUtils.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestIOUtils.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.IOUtils.java</file></fixedFiles></bug><bug fixdate="2015-01-25 04:15:23" id="11419" opendate="2014-12-17 05:24:17"><buginformation><summary>[HADOOP-11419] improve hadoop-maven-plugins - ASF JIRA</summary><description>little inconsistencies: property for Maven Plugin Tools http://maven.apache.org/plugin-tools/maven-plugin-plugin/ separate from Maven core, and use lates Plugin Tools version 3.3 better execution configuration for maven-plugin-plugin to avoid twice execution default value for protoc command as parameter configuration instead of code</description></buginformation><fixedFiles><file>hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.versioninfo.VersionInfoMojo.java</file><file>hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.protoc.ProtocMojo.java</file><file>hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.util.FileSetUtils.java</file><file>gelog.hadoop.hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.protoc.ProtocMojo.java</file><file>gelog.hadoop.hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.util.FileSetUtils.java</file><file>gelog.hadoop.hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.versioninfo.VersionInfoMojo.java</file></fixedFiles></bug><bug fixdate="2014-12-17 06:35:04" id="11416" opendate="2014-12-17 01:04:00"><buginformation><summary>[HADOOP-11416] Move ChunkedArrayList into hadoop-common - ASF JIRA</summary><description>Move ChunkedArrayList into hadoop-common so that it can be used by classes in hadoop-common, not just hdfs</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.util.TestChunkedArrayList.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.util.TestChunkedArrayList.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.util.ChunkedArrayList.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ChunkedArrayList.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.util.ChunkedArrayList.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.util.TestChunkedArrayList.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.java</file></fixedFiles></bug><bug fixdate="2014-12-23 10:32:38" id="11399" opendate="2014-07-15 12:13:00"><buginformation><summary>[HADOOP-11399] Java Configuration file and .xml files should be automatically cross-compared - ASF JIRA</summary><description>Update common in order to allow automatic comparison of Java Configuration classes and xxx-default.xml files within a unit test. Changes here will be used in downstream JIRAs.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.conf.TestConfigurationFieldsBase.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.test.java.org.apache.hadoop.mapreduce.TestMRJobConfigFields.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.java</file></fixedFiles></bug><bug fixdate="2015-01-05 04:10:48" id="11390" opendate="2014-12-11 11:44:06"><buginformation><summary>[HADOOP-11390] Metrics 2 ganglia provider to include hostname in unresolved address problems - ASF JIRA</summary><description>When metrics2/ganglia gets an unresolved hostname it doesn't include the hostname in question, making it harder to track down</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink.java</file></fixedFiles></bug><bug fixdate="2014-11-20 07:07:22" id="11323" opendate="2014-11-20 10:32:31"><buginformation><summary>[HADOOP-11323] WritableComparator#compare keeps reference to byte array - ASF JIRA</summary><description>When the default compare is used on a WritableComparator a reference to the second passed in byte array is kept in the buffer. Since WritableComparator keeps a reference to the buffer the byte will never be garbage collected. This can lead to a higher heap use than needed. The buffer should drop the reference to the byte array passed in. We can null out the byte array reference since the buffer is a private variable for the class.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableComparator.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.WritableComparator.java</file></fixedFiles></bug><bug fixdate="2014-12-02 07:13:04" id="11301" opendate="2014-11-12 11:48:19"><buginformation><summary>[HADOOP-11301] [optionally] update jmx cache to drop old metrics - ASF JIRA</summary><description>MetricsSourceAdapter::updateJmxCache() skips updating the info cache if no new metric is added since last time: &#13;
      int oldCacheSize = attrCache.size();&#13;
      int newCacheSize = updateAttrCache();&#13;
      if (oldCacheSize &lt; newCacheSize) {&#13;
        updateInfoCache();&#13;
      }&#13;
 This behavior is not desirable in some applications. For example nntop ( HDFS-6982) reports the top users via jmx. The list is updated after each report. The previously reported top users hence should be removed from the cache upon each report request. In our production run of nntop we made a change to ignore the size check and always perform updateInfoCache. I am planning to submit a patch including this change. The feature can be enabled by a configuration parameter.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.metrics2.impl.TestMetricsSourceAdapter.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.metrics2.impl.TestMetricsSourceAdapter.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsConfig.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.java</file></fixedFiles></bug><bug fixdate="2014-11-13 05:58:38" id="11291" opendate="2014-11-10 07:51:51"><buginformation><summary>[HADOOP-11291] Log the cause of SASL connection failures - ASF JIRA</summary><description>UGI#doAs will no longer log a PriviledgedActionException unless LOG.isDebugEnabled() == true. HADOOP-10015 made this change because it was decided that users calling UGI#doAs should be responsible for logging the error when catching an exception. Also, the log was confusing in certain situations (see more details in HADOOP-10015). However, as Daryn noted, this log message was very helpful in cases of debugging security issues. As an example, we would use to see this in the DN logs before HADOOP-10015: &#13;
2014-10-20 11:28:02,112 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs/hostA.com@REALM.COM (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Generic error (description in e-text) (60) - NO PREAUTH)]&#13;
2014-10-20 11:28:02,112 WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022&#13;
2014-10-20 11:28:02,112 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs/hostA.com@REALM.COM (auth:KERBEROS) cause:java.io.IOException: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022&#13;
 After the fix went in, the DN was upgraded, and only logs: &#13;
2014-10-20 14:11:40,712 WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022&#13;
2014-10-20 14:11:40,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: hostB.com/101.01.010:8022&#13;
 It'd be good to add more logging information about the cause of a SASL connection failure. Thanks to Harsh J for reporting this.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.java</file></fixedFiles></bug><bug fixdate="2015-01-16 10:17:12" id="11261" opendate="2014-11-03 10:03:17"><buginformation><summary>[HADOOP-11261] Set custom endpoint for S3A - ASF JIRA</summary><description>Use a config setting to allow customizing the used AWS region. It also enables using a custom url pointing to an S3-compatible object store.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.test.java.org.apache.hadoop.fs.s3a.TestS3AConfiguration.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.Constants.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.Constants.java</file></fixedFiles></bug><bug fixdate="2014-10-24 10:24:35" id="11231" opendate="2014-10-24 07:41:34"><buginformation><summary>[HADOOP-11231] Remove dead code in ServletUtil - ASF JIRA</summary><description>Some code in ServletUtil is dead after the JSP UI is phased out. This jira propose to clean them up.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ServletUtil.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ServletUtil.java</file></fixedFiles></bug><bug fixdate="2014-12-18 12:18:58" id="11188" opendate="2014-10-10 07:04:07"><buginformation><summary>[HADOOP-11188] hadoop-azure: automatically expand page blobs when they become full - ASF JIRA</summary><description>Right now, page blobs are initialized to a fixed size (fs.azure.page.blob.size) and cannot be expanded. This task is to make them automatically expand when they get to be nearly full. Design: if a write occurs that does not have enough room in the file to finish, then flush all preceding operations, extend the file, and complete the write. This will be synchronized (to have exclusive access) in access to PageBlobOutputStream so there won't be race conditions. The file will be extended by fs.azure.page.blob.extension.size bytes, which must be a multiple of 512. The internal default for fs.azure.page.blob.extension size will be 128 * 1024 * 1024. The minimum extension size will be 4 * 1024 * 1024 which is the maximum write size, so the new write will finish. Extension will stop when the file size reaches 1TB. The final extension may be less than fs.azure.page.blob.extension.size if the remainder (1TB - current_file_size) is smaller than fs.azure.page.blob.extension.size. An alternative to this is to make the default size 1TB. This is much simpler to implement. It's a one-line change. Or even simpler, don't change it at all because it is adequate for HBase. Rationale for this file size extension feature: 1) be able to download files to local disk easily with CloudXplorer and similar tools. Downloading a 1TB page blob is not practical if you don't have 1TB disk space since on the local side it expands to the full file size, locally filled with zeros where there is no valid data. 2) don't make customers uncomfortable when they see large 1TB files. They often ask if they have to pay for it, even though they only pay for the space actually used in the page blob. I think rationale 2 is a relatively minor issue, because 98% of customers for HBase will never notice. They will just use it and not look at what kind of files are used for the logs. They don't pay for the unused space, so it is not a problem for them. We can document this. Also, if they use hadoop fs -ls, they will see the actual size of the files since I put in a fix for that. Rationale 1 is a minor issue because you cannot interpret the data on your local file system anyway due to the data format. So really, the only reason to copy data locally in its binary format would be if you are moving it around or archiving it. Copying a 1TB page blob from one location in the cloud to another is pretty fast with smart copy utilities that don't actually move the 0-filled parts of the file. Nevertheless, this is a convenience feature for users. They won't have to worry about setting fs.azure.page.blob.size under normal circumstances and can make the files grow as big as they want. If we make the change to extend the file size on the fly, that introduces new possible error or failure modes for HBase. We should included retry logic.</description></buginformation><fixedFiles><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestReadAndSeekPageBlobAfterWrite.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobOutputStream.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.PageBlobOutputStream.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.TestReadAndSeekPageBlobAfterWrite.java</file></fixedFiles></bug><bug fixdate="2014-11-26 12:14:55" id="11173" opendate="2014-10-08 12:50:08"><buginformation><summary>[HADOOP-11173] Improve error messages for some KeyShell commands - ASF JIRA</summary><description>A few KeyShell commands don't print the exception messages and just swallow the exception, resulting in a non-specific error message.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.KeyShell.java</file></fixedFiles></bug><bug fixdate="2014-10-07 10:57:56" id="11172" opendate="2014-09-05 04:29:18"><buginformation><summary>[HADOOP-11172] Improve error message in Shell#runCommand on OutOfMemoryError - ASF JIRA</summary><description>TestWebHdfsFileSystemContract failed with the following error. But the real reason is not. Instead, it's because of "max user processes" of ulimit setting is too low. Need a more informative error message here. &#13;
2014-09-04 20:09:08,396 ERROR mortbay.log (Slf4jLog.java:warn(87)) - /webhdfs/v1/test/testSeek/zero&#13;
java.lang.OutOfMemoryError: unable to create new native thread&#13;
        at java.lang.Thread.start0(Native Method)&#13;
        at java.lang.Thread.start(Thread.java:693)&#13;
        at org.apache.hadoop.hdfs.LeaseRenewer.put(LeaseRenewer.java:320)&#13;
        at org.apache.hadoop.hdfs.DFSClient.beginFileLease(DFSClient.java:789)&#13;
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1605)&#13;
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1526)&#13;
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:234)&#13;
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.access$000(DatanodeWebHdfsMethods.java:86)&#13;
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:205)&#13;
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:202)&#13;
        at java.security.AccessController.doPrivileged(Native Method)&#13;
        at javax.security.auth.Subject.doAs(Subject.java:415)&#13;
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)&#13;
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:202)&#13;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#13;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&#13;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#13;
        at java.lang.reflect.Method.invoke(Method.java:606)&#13;
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)&#13;
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)&#13;
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)&#13;
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)&#13;
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)&#13;
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)&#13;
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)&#13;
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)&#13;
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)&#13;
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)&#13;
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)&#13;
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)&#13;
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)&#13;
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)&#13;
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)&#13;
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)&#13;
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)&#13;
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)&#13;
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1203)&#13;
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)&#13;
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)&#13;
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)&#13;
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)&#13;
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)&#13;
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)&#13;
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)&#13;
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)&#13;
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)&#13;
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)&#13;
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)&#13;
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)&#13;
        at org.mortbay.jetty.Server.handle(Server.java:326)&#13;
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)&#13;
        at org.mortbay.jetty.HttpConnection$RequestHandler.messageComplete(HttpConnection.java:959)&#13;
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:792)&#13;
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)&#13;
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)&#13;
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)&#13;
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.Shell.java</file></fixedFiles></bug><bug fixdate="2015-01-17 06:41:18" id="11171" opendate="2014-10-07 03:00:59"><buginformation><summary>[HADOOP-11171] Enable using a proxy server to connect to S3a. - ASF JIRA</summary><description>This exposes the AWS SDK config for a proxy (host and port) to s3a through config settings.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.test.java.org.apache.hadoop.fs.s3a.TestS3AConfiguration.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file><file>hadoop-tools.hadoop-aws.src.test.java.org.apache.hadoop.fs.s3a.TestS3AConfiguration.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFileSystem.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.Constants.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.Constants.java</file></fixedFiles></bug></bugrepository>