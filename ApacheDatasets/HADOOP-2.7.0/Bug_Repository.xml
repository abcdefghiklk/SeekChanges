<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository><bug fixdate="2015-04-17 07:00:35" id="11837" opendate="2015-04-15 08:43:59"><buginformation><summary>[HADOOP-11837] AuthenticationFilter should destroy SignerSecretProvider in Tomcat deployments - ASF JIRA</summary><description>AuthenticationFilter creates SignerSecretProvider if the filter is initialized through Tomcat. The SignerSecretProvider needs to be properly destroyed.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file></fixedFiles></bug><bug fixdate="2015-04-09 06:58:55" id="11815" opendate="2015-04-07 07:47:53"><buginformation><summary>[HADOOP-11815] HttpServer2 should destroy SignerSecretProvider when it stops - ASF JIRA</summary><description>It is observed that MRAppMaster JVM hungs after unregistered with ResourceManager.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.java</file></fixedFiles></bug><bug fixdate="2015-03-30 07:09:49" id="11761" opendate="2015-03-27 12:32:02"><buginformation><summary>[HADOOP-11761] Fix findbugs warnings in org.apache.hadoop.security.authentication - ASF JIRA</summary><description>As discovered in HADOOP-11748, we need to fix the findbugs warnings in org.apache.hadoop.security.authentication.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.java</file></fixedFiles></bug><bug fixdate="2015-04-02 01:10:14" id="11757" opendate="2015-03-25 08:00:33"><buginformation><summary>[HADOOP-11757] NFS gateway should shutdown when it can't start UDP or TCP server - ASF JIRA</summary><description>Unlike the Portmap, Nfs3 class does shutdown when the service can't start.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpServer.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpServer.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleTcpServer.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.nfs3.Nfs3Base.java</file><file>a.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.java</file><file>a.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.nfs3.Nfs3Base.java</file><file>a.hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleTcpServer.java</file></fixedFiles></bug><bug fixdate="2015-03-30 07:46:06" id="11754" opendate="2015-03-26 05:22:54"><buginformation><summary>[HADOOP-11754] RM fails to start in non-secure mode due to authentication filter failure - ASF JIRA</summary><description>RM fails to start in the non-secure mode with the following exception: 2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret&#13;
2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}&#13;
javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret&#13;
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)&#13;
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)&#13;
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)&#13;
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)&#13;
	at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)&#13;
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)&#13;
	at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)&#13;
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)&#13;
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)&#13;
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)&#13;
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)&#13;
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)&#13;
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)&#13;
	at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)&#13;
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)&#13;
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)&#13;
	at org.mortbay.jetty.Server.doStart(Server.java:224)&#13;
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)&#13;
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)&#13;
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)&#13;
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)&#13;
Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret&#13;
	at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)&#13;
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)&#13;
	... 23 more&#13;
...&#13;
2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager&#13;
org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server&#13;
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)&#13;
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)&#13;
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)&#13;
Caused by: java.io.IOException: Problem in starting http server. Server handlers failed&#13;
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)&#13;
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)&#13;
	... 4 more&#13;
 This is likely a regression introduced by HADOOP-10670.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.java</file><file>a.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.server.TestAuthenticationFilter.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSUtil.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.AuthenticationFilterInitializer.java</file><file>....server.AuthenticationFilter.java</file><file>a.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.server.TestAuthenticationFilter.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.http.HttpServer2.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.AuthenticationFilterInitializer.java</file></fixedFiles></bug><bug fixdate="2015-03-27 12:36:20" id="11748" opendate="2015-03-25 06:24:20"><buginformation><summary>[HADOOP-11748] The secrets of auth cookies should not be specified in configuration in clear text - ASF JIRA</summary><description>Based on the discussion on HADOOP-10670, this jira proposes to remove StringSecretProvider as it opens up possibilities for misconfiguration and security vulnerabilities. My understanding is that the use case of inlining the secret is never supported. The property is used to pass the secret internally. The way it works before HADOOP-10868 is the following: Users specify the initializer of the authentication filter in the configuration. AuthenticationFilterInitializer reads the secret file. The server will not start if the secret file does not exists. The initializer will set the property if it read the file correctly. *There is no way to specify the secret in the configuration out-of-the-box â€“ the secret is always overwritten by AuthenticationFilterInitializer.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.StringSignerSecretProvider.java</file><file>a.hadoop-hdfs-project.hadoop-hdfs-httpfs.src.test.java.org.apache.hadoop.fs.http.server.TestHttpFSServer.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.util.StringSignerSecretProviderCreator.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.server.TestAuthenticationFilter.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>a.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.server.TestAuthenticationFilter.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.util.StringSignerSecretProvider.java</file><file>a.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AuthenticationFilter.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs-httpfs.src.test.java.org.apache.hadoop.fs.http.server.TestHttpFSServer.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.StringSignerSecretProvider.java</file><file>....server.AuthenticationFilter.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.test.java.org.apache.hadoop.fs.http.server.TestHttpFSServer.java</file><file>hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.server.TestAuthenticationFilter.java</file><file>a.hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.StringSignerSecretProvider.java</file></fixedFiles></bug><bug fixdate="2015-03-18 02:43:24" id="11722" opendate="2015-03-16 09:39:07"><buginformation><summary>[HADOOP-11722] Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted - ASF JIRA</summary><description>The delete node code in ZKDelegationTokenSecretManager is as follows :        while(zkClient.checkExists().forPath(nodeRemovePath) != null){&#13;
          zkClient.delete().guaranteed().forPath(nodeRemovePath);&#13;
       }&#13;
 When instances of a Service using ZKDelegationTokenSecretManager try deleting a node simutaneously, It is possible that all of them enter into the while loop in which case, all peers will try to delete the node.. Only 1 will succeed and the rest will throw an exception.. which will bring down the node. The Exception is as follows : 2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception&#13;
java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28&#13;
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)&#13;
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)&#13;
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)&#13;
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28&#13;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)&#13;
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)&#13;
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)&#13;
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)&#13;
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)&#13;
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)&#13;
	at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)&#13;
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)&#13;
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)&#13;
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)&#13;
	... 4 more</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.java</file></fixedFiles></bug><bug fixdate="2015-03-17 07:11:11" id="11720" opendate="2015-03-16 03:38:16"><buginformation><summary>[HADOOP-11720] [JDK8] Fix javadoc errors caused by incorrect or illegal tags in hadoop-tools - ASF JIRA</summary><description>"mvn package -Pdist -DskipTests" fails with JDK8, caused by incorrect or illegal tags in doc comments.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.util.DistCpUtils.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3native.NativeS3FileSystem.java</file><file>hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.record.Buffer.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFastOutputStream.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3native.NativeS3FileSystem.java</file><file>hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.util.DistCpUtils.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>gelog.hadoop.hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.SimpleCopyListing.java</file><file>hadoop-tools.hadoop-ant.src.main.java.org.apache.hadoop.ant.DfsTask.java</file><file>gelog.hadoop.hadoop-tools.hadoop-ant.src.main.java.org.apache.hadoop.ant.DfsTask.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.record.Buffer.java</file><file>gelog.hadoop.hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.CopyListingFileStatus.java</file><file>gelog.hadoop.hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.S3FileSystem.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3a.S3AFastOutputStream.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>hadoop-tools.hadoop-aws.src.main.java.org.apache.hadoop.fs.s3.S3FileSystem.java</file><file>hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.SimpleCopyListing.java</file><file>hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.CopyListingFileStatus.java</file><file>hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.record.Utils.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.record.Utils.java</file></fixedFiles></bug><bug fixdate="2015-03-11 09:45:55" id="11693" opendate="2015-03-05 11:19:13"><buginformation><summary>[HADOOP-11693] Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. - ASF JIRA</summary><description>One of our customers' production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it. However, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state. &#13;
2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:&#13;
ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller&#13;
Cause:&#13;
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)&#13;
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)&#13;
	at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)&#13;
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)&#13;
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)&#13;
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)&#13;
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.&#13;
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)&#13;
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)&#13;
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)&#13;
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)&#13;
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)&#13;
	... 8 more&#13;
&#13;
2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN&#13;
java.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry&#13;
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)&#13;
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)&#13;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&#13;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)&#13;
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)&#13;
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)&#13;
	at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)&#13;
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)&#13;
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)&#13;
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)&#13;
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)&#13;
	... 4 more&#13;
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.&#13;
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)&#13;
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)&#13;
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)&#13;
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)&#13;
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)&#13;
	... 11 more&#13;
&#13;
Sun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338&#13;
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)&#13;
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)&#13;
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)&#13;
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)&#13;
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)&#13;
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)&#13;
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)&#13;
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)&#13;
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
 When archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled.</description></buginformation><fixedFiles><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterface.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.MockStorageInterface.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterface.java</file><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.StorageInterfaceImpl.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>a.hadoop-tools.hadoop-azure.src.test.java.org.apache.hadoop.fs.azure.MockStorageInterface.java</file></fixedFiles></bug><bug fixdate="2015-03-09 06:09:23" id="11686" opendate="2015-03-07 05:05:29"><buginformation><summary>[HADOOP-11686] MiniKDC cannot change ORG_NAME or ORG_DOMAIN - ASF JIRA</summary><description>We addressing HBASE-7781 I found that I can not change the default realm EXAMPLE.COM. I will get a 'Client not found in Kerberos database (6) - Client not found in Kerberos database' if I change it. This is because we do not set searchBaseDn when starting kdc, so it always search from ServerDNConstants.USER_EXAMPLE_COM_DN which is 'ou=users,dc=example,dc=com'.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-minikdc.src.main.java.org.apache.hadoop.minikdc.MiniKdc.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-minikdc.src.main.java.org.apache.hadoop.minikdc.MiniKdc.java</file><file>....java.org.apache.hadoop.minikdc.MiniKdc.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-minikdc.src.test.java.org.apache.hadoop.minikdc.TestChangeOrgNameAndDomain.java</file></fixedFiles></bug><bug fixdate="2015-03-05 07:39:07" id="11674" opendate="2015-03-05 05:15:42"><buginformation><summary>[HADOOP-11674] oneByteBuf in CryptoInputStream and CryptoOutputStream should be non static - ASF JIRA</summary><description>A common optimization in the io classes for Input/Output Streams is to save a single length-1 byte array to use in single byte read/write calls. CryptoInputStream and CryptoOutputStream both attempt to follow this practice but mistakenly mark the array as static. That means that only a single instance of each can be present in a JVM safely.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoOutputStream.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoInputStream.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoInputStream.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CryptoOutputStream.java</file></fixedFiles></bug><bug fixdate="2015-03-04 05:49:53" id="11666" opendate="2015-03-03 11:21:34"><buginformation><summary>[HADOOP-11666] Revert the format change of du output introduced by HADOOP-6857 - ASF JIRA</summary><description>HADOOP-6857 did two things about `du` at the same time. Fix a bug for querying snapshottable directory Change the output format (incompatible change) This issue is to revert the latter from branch-2 for keeping compatibility. The bug fix is left.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.FsUsage.java</file><file>gelog.hadoop.hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.TestDFSShell.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.TestDFSShell.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.FsUsage.java</file></fixedFiles></bug><bug fixdate="2015-02-25 05:16:03" id="11629" opendate="2015-02-24 07:46:35"><buginformation><summary>[HADOOP-11629] WASB filesystem should not start BandwidthGaugeUpdater if fs.azure.skip.metrics set to true - ASF JIRA</summary><description>In Hadoop-11248 we added configuration "fs.azure.skip.metrics". If set to true, we do not register Azure FileSystem metrics with the metrics system. However, BandwidthGaugeUpdater object is still created in AzureNativeFileSystemStore, resulting in unnecessary threads being spawned. Under heavy load the system could be busy dealing with these threads and GC has to work on removing the thread objects. E.g. When multiple WebHCat clients submitting jobs to WebHCat server, we observed that the WebHCat server spawns ~400 daemon threads, which slows down the server and sometimes cause timeout.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file></fixedFiles></bug><bug fixdate="2015-02-24 05:27:04" id="11619" opendate="2015-02-21 03:35:20"><buginformation><summary>[HADOOP-11619] FTPFileSystem should override getDefaultPort - ASF JIRA</summary><description>FTPFileSystem should override FileSystem#getDefaultPort to return FTP.DEFAULT_PORT</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ftp.FTPFileSystem.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.ftp.TestFTPFileSystem.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.ftp.FTPFileSystem.java</file></fixedFiles></bug><bug fixdate="2015-03-10 09:49:02" id="11618" opendate="2015-02-21 03:13:18"><buginformation><summary>[HADOOP-11618] DelegateToFileSystem erroneously uses default FS's port in constructor - ASF JIRA</summary><description>DelegateToFileSystem constructor has the following code: &#13;
    super(theUri, supportedScheme, authorityRequired,&#13;
        FileSystem.getDefaultUri(conf).getPort());&#13;
 The default port should be taken from theFsImpl instead. &#13;
    super(theUri, supportedScheme, authorityRequired,&#13;
        theFsImpl.getDefaultPort());</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.DelegateToFileSystem.java</file></fixedFiles></bug><bug fixdate="2015-03-24 12:01:01" id="11609" opendate="2015-02-18 02:41:40"><buginformation><summary>[HADOOP-11609] Correct credential commands info in CommandsManual.html#credential - ASF JIRA</summary><description>"-i" is not supported, so would you remove "-i",,, " -v" should be undocumented. The option is used only by test. create alias [-v value][-provider provider-path]	Prompts the user for a credential to be stored as the given alias when a value is not provided via -v. The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.&#13;
delete alias [-i][-provider provider-path]	Deletes the credential with the provided alias and optionally warns the user when --interactive is used. The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.&#13;
list [-provider provider-path]	Lists all of the credential aliases The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file></fixedFiles></bug><bug fixdate="2015-03-03 02:25:02" id="11605" opendate="2015-02-17 08:23:26"><buginformation><summary>[HADOOP-11605] FilterFileSystem#create with ChecksumOpt should propagate it to wrapped FS - ASF JIRA</summary><description>Current create code &#13;
  @Override&#13;
  public FSDataOutputStream create(Path f,&#13;
        FsPermission permission,&#13;
        EnumSet&lt;CreateFlag&gt; flags,&#13;
        int bufferSize,&#13;
        short replication,&#13;
        long blockSize,&#13;
        Progressable progress,&#13;
        ChecksumOpt checksumOpt) throws IOException {&#13;
    return fs.create(f, permission,&#13;
      flags, bufferSize, replication, blockSize, progress);&#13;
  }&#13;
 does not propagate ChecksumOpt. However, it should be up to the wrapped FS implementation (default is to ignore).</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FilterFileSystem.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FilterFileSystem.java</file></fixedFiles></bug><bug fixdate="2015-02-20 09:12:12" id="11604" opendate="2015-02-16 11:49:37"><buginformation><summary>[HADOOP-11604] Prevent ConcurrentModificationException while closing domain sockets during shutdown of DomainSocketWatcher thread. - ASF JIRA</summary><description>Our product cluster hit the Xceiver limit even w/ HADOOP-10404 &amp; HADOOP-11333, i found it was caused by DomainSocketWatcher.watcherThread gone. Attached is a possible fix, please review, thanks</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.net.unix.TestDomainSocketWatcher.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.net.unix.TestDomainSocketWatcher.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocketWatcher.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.unix.DomainSocketWatcher.java</file></fixedFiles></bug><bug fixdate="2015-03-09 11:01:39" id="11602" opendate="2015-02-16 11:31:56"><buginformation><summary>[HADOOP-11602] Fix toUpperCase/toLowerCase to use Locale.ENGLISH - ASF JIRA</summary><description>String#toLowerCase()/toUpperCase() without a locale argument can occur unexpected behavior based on the locale. It's written in Javadoc: For instance, "TITLE".toLowerCase() in a Turkish locale returns "t\u0131tle", where '\u0131' is the LATIN SMALL LETTER DOTLESS I character This issue is derived from HADOOP-10101.</description></buginformation><fixedFiles><file>hadoop-hdfs-project.hadoop-hdfs.src.test.java.org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotManager.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.XAttrHelper.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapred.Task.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.TypeConverter.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.slive.OperationOutput.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.WebServices.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.StorageType.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl.java</file><file>hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.webapp.AppController.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.GetConf.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsVisitorFactory.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.SSLHostnameVerifier.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.io.FileBench.java</file><file>hadoop-tools.hadoop-gridmix.src.main.java.org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ipc.TestSaslRPC.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.util.TestWinUtils.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.fs.http.server.HttpFSParametersProvider.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.SSLFactory.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.test.java.org.apache.hadoop.mapreduce.TestTypeConverter.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.XAttrCommands.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.lib.db.DBInputFormat.java</file><file>hadoop-tools.hadoop-openstack.src.test.java.org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract.java</file><file>hadoop-common-project.hadoop-auth.src.test.java.org.apache.hadoop.security.authentication.util.TestKerberosUtil.java</file><file>hadoop-tools.hadoop-rumen.src.main.java.org.apache.hadoop.tools.rumen.LoggedTaskAttempt.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.lib.service.hadoop.FileSystemAccessService.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsSystemImpl.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-common.src.main.java.org.apache.hadoop.mapreduce.v2.util.MRApps.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.JavaKeyStoreProvider.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.StorageLocation.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogOp.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.test.TimedOutTestsListener.java</file><file>hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.server.AltKerberosAuthenticationHandler.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.ipc.TestIPC.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-examples.src.main.java.org.apache.hadoop.examples.DBCountPageView.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.mapred.TestMapRed.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.java</file><file>hadoop-common-project.hadoop-annotations.src.main.java.org.apache.hadoop.classification.tools.StabilityOptions.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.common.HdfsServerConstants.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.slive.SliveTest.java</file><file>hadoop-maven-plugins.src.main.java.org.apache.hadoop.maven.plugin.versioninfo.VersionInfoMojo.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.FileSystemContractBaseTest.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.record.compiler.generated.Rcc.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.WebHdfsFileSystem.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.slive.Constants.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.conf.Configuration.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.AuthFilter.java</file><file>hadoop-tools.hadoop-streaming.src.main.java.org.apache.hadoop.streaming.Environment.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceWeights.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry.src.main.java.org.apache.hadoop.registry.client.binding.RegistryUtils.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.WhitelistBasedResolver.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.tools.CLI.java</file><file>hadoop-tools.hadoop-rumen.src.main.java.org.apache.hadoop.tools.rumen.LoggedTask.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.cli.NodeCLI.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.shell.find.Name.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SecurityUtil.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.record.compiler.CppGenerator.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.slive.OperationData.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.ParamFilter.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.lib.server.Server.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.StorageType.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.compress.CompressionCodecFactory.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.StringUtils.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.TestDFSIO.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.lib.wsrs.ParametersProvider.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite.java</file><file>hadoop-tools.hadoop-rumen.src.main.java.org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.util.TestStringUtils.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.metrics2.impl.MetricsConfig.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.record.compiler.CGenerator.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslPropertiesResolver.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.resources.EnumParam.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.CipherSuite.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.cli.ApplicationCLI.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.lib.wsrs.EnumSetParam.java</file><file>hadoop-tools.hadoop-rumen.src.main.java.org.apache.hadoop.tools.rumen.JobBuilder.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.hamlet.HamletGen.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.TestSecurityUtil.java</file><file>hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.fs.http.server.FSOperations.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices.java</file><file>hadoop-common-project.hadoop-auth.src.main.java.org.apache.hadoop.security.authentication.util.KerberosUtil.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.lib.wsrs.EnumParam.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.FSDownload.java</file><file>hadoop-tools.hadoop-extras.src.main.java.org.apache.hadoop.tools.DistCpV1.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.web.resources.EnumSetParam.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ComparableVersion.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-jobclient.src.test.java.org.apache.hadoop.fs.TestFileSystem.java</file><file>hadoop-tools.hadoop-distcp.src.main.java.org.apache.hadoop.tools.util.DistCpUtils.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.HdfsConstants.java</file><file>hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.permission.AclEntry.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.TestUserGroupInformation.java</file><file>hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-core.src.main.java.org.apache.hadoop.mapreduce.filecache.DistributedCache.java</file><file>hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.java</file><file>hadoop-hdfs-project.hadoop-hdfs-httpfs.src.main.java.org.apache.hadoop.fs.http.server.CheckUploadContentTypeFilter.java</file></fixedFiles></bug><bug fixdate="2015-02-19 09:27:11" id="11595" opendate="2015-02-14 04:59:52"><buginformation><summary>[HADOOP-11595] Add default implementation for AbstractFileSystem#truncate - ASF JIRA</summary><description>As Chris Nauroth commented in HADOOP-11510, we should add a default implementation for AbstractFileSystem#truncate to avoid backwards-compatibility</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.TestAfsCheckPath.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.AbstractFileSystem.java</file><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.fs.TestAfsCheckPath.java</file></fixedFiles></bug><bug fixdate="2015-02-12 06:31:25" id="11587" opendate="2015-02-12 03:44:41"><buginformation><summary>[HADOOP-11587] TestMapFile#testMainMethodMapFile creates test files in hadoop-common project root - ASF JIRA</summary><description>After running TestMapFile#testMainMethodMapFile, two files (data and index) and a directory remain in hadoop-common project root dir. hadoop-common-project$ git status Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) hadoop-common/mainMethodMapFile.mapfile/ ... hadoop-common-project$ tree hadoop-common/mainMethodMapFile.mapfile/ hadoop-common/mainMethodMapFile.mapfile/ â”œâ”€â”€ data â””â”€â”€ index The fix is to use "path" instead of "mainMethodMapFile.mapfile" as output file when calling MapFile.Main(). I will post a patch soon for it.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestMapFile.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.TestMapFile.java</file></fixedFiles></bug><bug fixdate="2015-02-18 07:20:24" id="11545" opendate="2015-02-04 10:28:43"><buginformation><summary>[HADOOP-11545] ArrayIndexOutOfBoundsException is thrown with "hadoop credential list -provider" - ASF JIRA</summary><description>Scenario: ======== Please run the following command . dn't give the provider path. [hdfs@host194 bin]$ ./hadoop credential list -provider&#13;
java.lang.ArrayIndexOutOfBoundsException: 2&#13;
        at org.apache.hadoop.security.alias.CredentialShell.init(CredentialShell.java:117)&#13;
        at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:63)&#13;
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)&#13;
        at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:427)</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.alias.TestCredShell.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.security.alias.TestCredShell.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.alias.CredentialShell.java</file></fixedFiles></bug><bug fixdate="2015-02-03 06:53:41" id="11529" opendate="2015-01-30 11:58:26"><buginformation><summary>[HADOOP-11529] Fix findbugs warnings in hadoop-archives - ASF JIRA</summary><description>NP	Possible null pointer dereference of reader in org.apache.hadoop.tools.HadoopArchives$HArchiveInputFormat.getSplits(JobConf, int) on exception path&#13;
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.close(): String.getBytes()&#13;
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.configure(JobConf): String.getBytes()&#13;
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.reduce(IntWritable, Iterator, OutputCollector, Reporter): String.getBytes()</description></buginformation><fixedFiles><file>a.hadoop-tools.hadoop-archives.src.main.java.org.apache.hadoop.tools.HadoopArchives.java</file><file>gelog.hadoop.hadoop-tools.hadoop-archives.src.main.java.org.apache.hadoop.tools.HadoopArchives.java</file></fixedFiles></bug><bug fixdate="2015-01-30 01:08:39" id="11523" opendate="2015-01-29 06:57:37"><buginformation><summary>[HADOOP-11523] StorageException complaining " no lease ID" when updating FolderLastModifiedTime in WASB - ASF JIRA</summary><description>In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's "last modified time" property. By default we do not acquire lease on this folder when updating its property and simply pass "null" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass "null" to it. &#13;
ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn't rename wasb://xxx/hbase/data/default/tdelrowtbl/3c842e8823c192d1028dc72ac3f22886/recovered.edits/0000000000000000015.temp to wasb://xxx/hbase/data/default/tdelrowtbl/3c842e8823c192d1028dc72ac3f22886/recovered.edits/0000000000000000015&#13;
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2558)&#13;
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2569)&#13;
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.updateParentFolderLastModifiedTime(NativeAzureFileSystem.java:2016)&#13;
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1983)&#13;
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1161)&#13;
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1121)&#13;
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)&#13;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)&#13;
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)&#13;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&#13;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&#13;
	at java.lang.Thread.run(Thread.java:745)</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file><file>a.hadoop-tools.hadoop-azure.src.main.java.org.apache.hadoop.fs.azure.NativeAzureFileSystem.java</file></fixedFiles></bug><bug fixdate="2015-02-10 07:23:32" id="11512" opendate="2015-01-27 02:07:12"><buginformation><summary>[HADOOP-11512] Use getTrimmedStrings when reading serialization keys - ASF JIRA</summary><description>In the file hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java, we grab the IO_SERIALIZATIONS_KEY config as Configuration#getStrings(â€¦) which does not trim the input. This could cause confusing user issues if someone manually overrides the key in the XML files/Configuration object without using the dynamic approach. The call should instead use Configuration#getTrimmedStrings(â€¦), so the whitespace is trimmed before the class names are searched on the classpath.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.serializer.TestSerializationFactory.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.serializer.SerializationFactory.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.serializer.SerializationFactory.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.serializer.SerializationFactory.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.serializer.TestSerializationFactory.java</file><file>hadoop-common-project.hadoop-common.src.test.java.org.apache.hadoop.io.serializer.TestSerializationFactory.java</file></fixedFiles></bug><bug fixdate="2015-01-26 11:38:11" id="11509" opendate="2015-01-26 03:50:01"><buginformation><summary>[HADOOP-11509] change parsing sequence in GenericOptionsParser to parse -D parameters first - ASF JIRA</summary><description>In GenericOptionsParser, we need to parse -D parameter first. In that case, the user input parameter (through -D) can be set into configuration object earlier and used to process other parameters.</description></buginformation><fixedFiles><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.GenericOptionsParser.java</file></fixedFiles></bug><bug fixdate="2015-01-23 07:39:09" id="11507" opendate="2015-01-23 02:15:06"><buginformation><summary>[HADOOP-11507] Hadoop RPC Authentication problem with different user locale - ASF JIRA</summary><description>When I try to use hadoop mapreduce framework with Turkish locale that set default mapreduce.map.java.opts and mapreduce.reduce.java.opts to -Duser.language=tr. It throws exception. After Long research, i found a little bug in org.apache.hadoop.security.SaslPropertiesResolver line 68. default setting of hadoop.rpc.protection is authentication. When i make locale Turkish authentication parameter with toUpperCase become AUTHENTÄ°CATÄ°ON. Please attention to dotted big i. This is a Turkish letter. It is very similar big i, it just have addition dot. IMHO who-one use upper or lowercase method for settings, it should use Locale.ENGLISH. I created a patch file. But I could not test it. I can not find which tests should i run. If you show me, I will be glad. BTW I think, Hadoop Tests should be run different locale. {block} ERROR operation.Operation: Error running hive query: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:147) at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69) at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614) at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502) at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) {block} p.s. Exception is generated by a Hive query. It can be generated by any map reduce job.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslPropertiesResolver.java</file><file>hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslPropertiesResolver.java</file></fixedFiles></bug><bug fixdate="2015-01-22 03:51:34" id="11500" opendate="2015-01-22 12:34:29"><buginformation><summary>[HADOOP-11500] InputStream is left unclosed in ApplicationClassLoader - ASF JIRA</summary><description>InputStream is = null;&#13;
    try {&#13;
      is = ApplicationClassLoader.class.getClassLoader().&#13;
          getResourceAsStream(PROPERTIES_FILE);&#13;
 The InputStream is not closed in the static block.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ApplicationClassLoader.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.util.ApplicationClassLoader.java</file></fixedFiles></bug><bug fixdate="2015-01-26 04:58:02" id="11499" opendate="2015-01-22 12:06:03"><buginformation><summary>[HADOOP-11499] Check of executorThreadsStarted in ValueQueue#submitRefillTask() evades lock acquisition - ASF JIRA</summary><description>if (!executorThreadsStarted) {&#13;
      synchronized (this) {&#13;
        // To ensure all requests are first queued, make coreThreads =&#13;
        // maxThreads&#13;
        // and pre-start all the Core Threads.&#13;
        executor.prestartAllCoreThreads();&#13;
        executorThreadsStarted = true;&#13;
      }&#13;
    }&#13;
 It is possible that two threads executing the above code both see executorThreadsStarted as being false, leading to executor.prestartAllCoreThreads() called twice.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.ValueQueue.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.ValueQueue.java</file></fixedFiles></bug><bug fixdate="2015-02-02 06:51:48" id="11494" opendate="2015-01-20 08:08:42"><buginformation><summary>[HADOOP-11494] Lock acquisition on WrappedInputStream#unwrappedRpcBuffer may race with another thread - ASF JIRA</summary><description>In SaslRpcClient, starting at line 576: &#13;
    public int read(byte[] buf, int off, int len) throws IOException {&#13;
      synchronized(unwrappedRpcBuffer) {&#13;
        // fill the buffer with the next RPC message&#13;
        if (unwrappedRpcBuffer.remaining() == 0) {&#13;
          readNextRpcPacket();&#13;
        }&#13;
 readNextRpcPacket() may assign another ByteBuffer to unwrappedRpcBuffer, making the lock on previous ByteBuffer not useful.</description></buginformation><fixedFiles><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcClient.java</file></fixedFiles></bug><bug fixdate="2015-01-23 08:11:49" id="11482" opendate="2015-01-15 07:58:59"><buginformation><summary>[HADOOP-11482] Use correct UGI when KMSClientProvider is called by a proxy user - ASF JIRA</summary><description>Long Living clients of HDFS (For eg. OOZIE) use cached DFSClients which in turn use a cached KMSClientProvider to talk to KMS. Before an MR Job is run, the job client calls the DFClient.addDelegationTokens() method which calls addDelegationTokens() on the KMSClientProvider to get any delegation token associated to the user. Unfortunately, this call uses a cached DelegationTokenAuthenticationURL.Token instance which can cause the SignerSecretProvider implementation of the AuthenticationFilter at the KMS Server end to fail validation. Which results in the MR job itself failing.</description></buginformation><fixedFiles><file>gelog.hadoop.hadoop-common-project.hadoop-kms.src.test.java.org.apache.hadoop.crypto.key.kms.server.TestKMS.java</file><file>a.hadoop-common-project.hadoop-kms.src.test.java.org.apache.hadoop.crypto.key.kms.server.TestKMS.java</file><file>gelog.hadoop.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.java</file><file>a.hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.crypto.key.kms.KMSClientProvider.java</file></fixedFiles></bug></bugrepository>