<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository><bug fixdate="2015-01-21 11:58:00" id="6991" opendate="2015-01-16 05:01:40"><buginformation><summary>[SOLR-6991] Update to Apache TIKA 1.7 - ASF JIRA</summary><description>Apache TIKA 1.7 was released: https://dist.apache.org/repos/dist/release/tika/CHANGES-1.7.txt This is more or less a dependency update, so replacements. Not sure if we should do this for 5.0. In 5.0 we currently have the previous version, which was not yet released with Solr. If we now bring this into 5.0, we wouldn't have a new release 2 times. I can change the stuff this evening and let it bake in 5.x, so maybe we backport this.</description></buginformation><fixedFiles><file>solr.contrib.extraction.src.test.org.apache.solr.handler.extraction.ExtractingRequestHandlerTest.java</file><file>solr.contrib.dataimporthandler-extras.src.test.org.apache.solr.handler.dataimport.TestTikaEntityProcessor.java</file></fixedFiles></bug><bug fixdate="2015-01-22 02:47:50" id="6988" opendate="2015-01-16 06:29:58"><buginformation><summary>[SOLR-6988] Make stateformat=2 as default for Solr 5.0 - ASF JIRA</summary><description>If we want to do it, perhaps now is a good time. What do you guys think? Noble Paul, Shalin Shekhar Mangar, Tim Potter and Mark Miller Specially as SOLR-6554 is resolved and seems like batching is supported for the new stateformat, we should consider this.</description></buginformation><fixedFiles><file>solr.test-framework.src.java.org.apache.solr.cloud.AbstractFullDistribZkTestBase.java</file><file>solr.core.src.java.org.apache.solr.handler.admin.CollectionsHandler.java</file><file>solr.core.src.test.org.apache.solr.cloud.ExternalCollectionsTest.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file></fixedFiles></bug><bug fixdate="2015-01-10 06:10:24" id="6950" opendate="2015-01-10 02:51:50"><buginformation><summary>[SOLR-6950] Ensure TransactionLogs are closed with test ObjectReleaseTracker. - ASF JIRA</summary><description/></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.update.TransactionLog.java</file><file>solr.core.src.java.org.apache.solr.update.HdfsTransactionLog.java</file></fixedFiles></bug><bug fixdate="2015-01-15 04:20:10" id="6943" opendate="2015-01-09 06:58:58"><buginformation><summary>[SOLR-6943] HdfsDirectoryFactory should fall back to system props for most of it's config if it is not found in solrconfig.xml. - ASF JIRA</summary><description>The new server and config sets has undone the work I did to make hdfs easy out of the box. Rather than count on config for that, we should just allow most of this config to be specified at the sys property level. This improves the global cache config situation as well.</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.util.MockSolrResourceLoader.java</file><file>solr.core.src.test.org.apache.solr.util.MockCoreContainer.java</file><file>solr.core.src.test.org.apache.solr.cloud.hdfs.HdfsTestUtil.java</file><file>solr.core.src.test.org.apache.solr.core.HdfsDirectoryFactoryTest.java</file><file>solr.core.src.java.org.apache.solr.core.HdfsDirectoryFactory.java</file></fixedFiles></bug><bug fixdate="2015-01-07 05:40:01" id="6918" opendate="2015-01-06 09:28:59"><buginformation><summary>[SOLR-6918] No need to log exceptions (as warn) generated when creating MBean stats if the core is shutting down - ASF JIRA</summary><description>I'm seeing tons of this warning when distributed SolrCloud tests are shutting down ... I don't think we should log a WARN message if the exception is AlreadyClosedException since the core is closing [junit4] 2&gt; 31971 T37 oasc.JmxMonitoredMap$SolrDynamicMBean.getMBeanInfo WARN Could not getStatistics on info bean org.apache.solr.handler.ReplicationHandler org.apache.solr.common.SolrException: openNewSearcher called on closed core [junit4] 2&gt; at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1498) [junit4] 2&gt; at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1736) [junit4] 2&gt; at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1481) [junit4] 2&gt; at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1416) [junit4] 2&gt; at org.apache.solr.handler.ReplicationHandler.getIndexVersion(ReplicationHandler.java:558) [junit4] 2&gt; at org.apache.solr.handler.ReplicationHandler.getStatistics(ReplicationHandler.java:575) [junit4] 2&gt; at org.apache.solr.core.JmxMonitoredMap$SolrDynamicMBean.getMBeanInfo(JmxMonitoredMap.java:257) [junit4] 2&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getClassName(DefaultMBeanServerInterceptor.java:1804) [junit4] 2&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.safeGetClassName(DefaultMBeanServerInterceptor.java:1595) [junit4] 2&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.objectInstancesFromFilteredNamedObjects(DefaultMBeanServerInterceptor.java:1556) [junit4] 2&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryMBeansImpl(DefaultMBeanServerInterceptor.java:513) [junit4] 2&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryNames(DefaultMBeanServerInterceptor.java:526) [junit4] 2&gt; at com.sun.jmx.mbeanserver.JmxMBeanServer.queryNames(JmxMBeanServer.java:619) [junit4] 2&gt; at org.apache.solr.core.JmxMonitoredMap.clear(JmxMonitoredMap.java:120) [junit4] 2&gt; at org.apache.solr.core.SolrCore.close(SolrCore.java:1161) [junit4] 2&gt; at org.apache.solr.core.CoreContainer.unload(CoreContainer.java:691) [junit4] 2&gt; at org.apache.solr.handler.admin.CoreAdminHandler.handleUnloadAction(CoreAdminHandler.java:676) [junit4] 2&gt; at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestInternal(CoreAdminHandler.java:209) [junit4] 2&gt; at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:188) [junit4] 2&gt; at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:144) [junit4] 2&gt; at org.apache.solr.servlet.SolrDispatchFilter.handleAdminRequest(SolrDispatchFilter.java:738) [junit4] 2&gt; at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:254) [junit4] 2&gt; at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:201)</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.core.JmxMonitoredMap.java</file><file>solr.core.src.java.org.apache.solr.handler.ReplicationHandler.java</file></fixedFiles></bug><bug fixdate="2014-12-24 03:57:34" id="6879" opendate="2014-12-22 03:54:21"><buginformation><summary>[SOLR-6879] Have an option to disable autoAddReplicas temporarily for all collections - ASF JIRA</summary><description>When doing offline maintenance on my cluster I would like to disable autoAddReplicas since I don't want replicas being created on nodes during this time. It would be useful to have an option to enable/disable autoAddReplicas via an API. This API would disable autoAddReplicas: &#13;
admin/collections?action=CLUSTERPROP&amp;name=autoAddReplicas&amp;val=false&#13;
 Now when I want to enable it again the API call would look like this: &#13;
admin/collections?action=CLUSTERPROP&amp;name=autoAddReplicas&#13;
 This uses the CLUSTERPROP semantics of unsetting a property when the val param is not provided.</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.cloud.SharedFSAutoReplicaFailoverTest.java</file><file>solr.core.src.java.org.apache.solr.cloud.Overseer.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerAutoReplicaFailoverThread.java</file></fixedFiles></bug><bug fixdate="2014-12-17 05:34:21" id="6852" opendate="2014-12-15 09:21:41"><buginformation><summary>[SOLR-6852] SimplePostTool should no longer default to collection1 - ASF JIRA</summary><description>Solr no longer would be bootstrapped with "collection1" and so it no longer makes sense for the SimplePostTool to default to collection1 either. Without an explicit collection/core/url value, the call should just fail fast.</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.util.SimplePostTool.java</file><file>solr.core.src.test.org.apache.solr.util.SimplePostToolTest.java</file></fixedFiles></bug><bug fixdate="2014-12-15 11:12:29" id="6849" opendate="2014-12-14 10:57:54"><buginformation><summary>[SOLR-6849] RemoteSolrExceptions thrown from HttpSolrServer should include the URL of the remote host - ASF JIRA</summary><description>All very well telling me there was an error on a remote host, but it's difficult to work out what's wrong if it doesn't tell me which host the error was on...</description></buginformation><fixedFiles><file>a.solr.solrj.src.java.org.apache.solr.client.solrj.impl.HttpSolrServer.java</file></fixedFiles></bug><bug fixdate="2014-12-14 05:01:40" id="6827" opendate="2014-12-08 06:32:50"><buginformation><summary>[SOLR-6827] DateRangeField: support facet.range, exclusive range, DateMath - ASF JIRA</summary><description>DateRangeField can be made to be more compatible with TrieDateField: facet.range (thus faceting on date durations and date instances alike) exclusive range boundaries Solr DateMath syntax stored value should have resolved DateMath Not sure about this one but perhaps the XML / javabin can return a date instead of a string if the value is a date instance and not a range.</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.request.SimpleFacetsTest.java</file><file>solr.core.src.java.org.apache.solr.request.SimpleFacets.java</file><file>solr.core.src.java.org.apache.solr.schema.DateRangeField.java</file><file>solr.core.src.java.org.apache.solr.schema.AbstractSpatialFieldType.java</file><file>solr.core.src.test.org.apache.solr.schema.DateRangeFieldTest.java</file></fixedFiles></bug><bug fixdate="2015-01-19 10:59:25" id="6799" opendate="2014-11-27 02:48:25"><buginformation><summary>[SOLR-6799] Update Saxon-HE to 9.6.0-2 - ASF JIRA</summary><description>Our version of Saxon-HE has a corrupt manifest file - this has posed problems before but we worked around it somehow. Since trunk has moved to Java 8, this has come up again for me as I can no longer run tests in eclipse because it complains about the corrupt manifest. This has since been fixed, so we should update.</description></buginformation><fixedFiles><file>solr.contrib.map-reduce.src.test.org.apache.solr.hadoop.MorphlineMapperTest.java</file><file>solr.contrib.map-reduce.src.test.org.apache.solr.hadoop.MorphlineReducerTest.java</file><file>solr.contrib.map-reduce.src.test.org.apache.solr.hadoop.MorphlineBasicMiniMRTest.java</file><file>solr.contrib.morphlines-core.src.test.org.apache.solr.morphlines.solr.SolrMorphlineTest.java</file><file>solr.contrib.morphlines-core.src.test.org.apache.solr.morphlines.solr.SolrMorphlineZkTest.java</file><file>solr.contrib.morphlines-core.src.test.org.apache.solr.morphlines.solr.SolrMorphlineZkAvroTest.java</file><file>solr.contrib.morphlines-core.src.test.org.apache.solr.morphlines.solr.SolrMorphlineZkAliasTest.java</file><file>solr.contrib.map-reduce.src.test.org.apache.solr.hadoop.MorphlineGoLiveMiniMRTest.java</file></fixedFiles></bug><bug fixdate="2015-01-03 08:35:41" id="6797" opendate="2014-11-26 02:45:40"><buginformation><summary>[SOLR-6797] Add score=degrees|kilometers|miles for AbstractSpatialFieldType - ASF JIRA</summary><description>Annoyingly, the units="degrees" attribute is required for fields extending AbstractSpatialFieldType (e.g. RPT, BBox). And it doesn't really have any effect. I propose the following: Simply drop the attribute; ignore it if someone sets it to "degrees" (for back-compat). When using score="distance", or score=area or area2D (as seen in BBoxField) then use kilometers if geo=true, otherwise degrees. Add support for score=degrees|kilometers|miles|degrees</description></buginformation><fixedFiles><file>a.solr.core.src.java.org.apache.solr.schema.PointType.java</file><file>a.solr.core.src.java.org.apache.solr.schema.LatLonType.java</file><file>a.solr.core.src.java.org.apache.solr.search.SpatialFilterQParser.java</file><file>a.solr.core.src.java.org.apache.solr.schema.GeoHashField.java</file><file>a.solr.core.src.java.org.apache.solr.schema.DateRangeField.java</file><file>a.solr.core.src.java.org.apache.solr.schema.AbstractSpatialFieldType.java</file><file>a.lucene.spatial.src.java.org.apache.lucene.spatial.util.ShapeAreaValueSource.java</file><file>a.solr.core.src.test.org.apache.solr.search.TestSolr4Spatial.java</file><file>a.solr.core.src.java.org.apache.solr.schema.AbstractSpatialPrefixTreeFieldType.java</file><file>a.lucene.spatial.src.test.org.apache.lucene.spatial.bbox.TestBBoxStrategy.java</file><file>solr.core.src.java.org.apache.solr.search.SpatialFilterQParser.java</file><file>a.solr.core.src.java.org.apache.solr.schema.BBoxField.java</file><file>solr.core.src.test.org.apache.solr.search.TestSolr4Spatial.java</file><file>a.solr.core.src.java.org.apache.solr.schema.SpatialQueryable.java</file><file>lucene.spatial.src.java.org.apache.lucene.spatial.util.ShapeAreaValueSource.java</file><file>solr.core.src.java.org.apache.solr.util.DistanceUnits.java</file></fixedFiles></bug><bug fixdate="2014-11-19 04:20:37" id="6747" opendate="2014-11-16 08:29:42"><buginformation><summary>[SOLR-6747] Add an optional caching option as a workaround for SOLR-6586. - ASF JIRA</summary><description/></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.core.TestSolrDynamicMBean.java</file><file>solr.core.src.test.org.apache.solr.core.TestJmxMonitoredMap.java</file><file>solr.core.src.java.org.apache.solr.core.JmxMonitoredMap.java</file><file>solr.core.src.test.org.apache.solr.core.MockInfoMBean.java</file></fixedFiles></bug><bug fixdate="2014-12-23 04:34:09" id="6680" opendate="2014-10-30 04:26:09"><buginformation><summary>[SOLR-6680] DefaultSolrHighlighter can sometimes avoid CachingTokenFilter - ASF JIRA</summary><description>The DefaultSolrHighlighter (the most accurate one) is a bit over-eager to wrap the token stream in a CachingTokenFilter when hl.usePhraseHighlighter=true. This wastes memory, and it interferes with other optimizations – LUCENE-6034. Furthermore, the internal TermOffsetsTokenStream (used when TermVectors are used with this) wasn't properly delegating reset().</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.highlight.HighlighterTest.java</file><file>solr.core.src.java.org.apache.solr.highlight.DefaultSolrHighlighter.java</file></fixedFiles></bug><bug fixdate="2014-11-03 04:51:23" id="6670" opendate="2014-10-29 10:40:47"><buginformation><summary>[SOLR-6670] change BALANCESLICEUNIQUE to BALANCESHARDUNIQUE - ASF JIRA</summary><description>JIRA for Jan's comments on SOLR-6513: I thought we agreed to prefer the term "shard" over "slice", so I think we should do this for this API as well. The only place in our refguide we use the word "slice" is in How SolrCloud Works [1] and that description is disputed. The refguide explanation of what a shard is can be found in Shards and Indexing Data in SolrCloud [2], quoting: When your data is too large for one node, you can break it up and store it in sections by creating one or more shards. Each is a portion of the logical index, or core, and it's the set of all nodes containing that section of the index. So I'm proposing a rename of this API to BALANCESHARDUNIQUE and a rewrite of [1]. [1] https://cwiki.apache.org/confluence/display/solr/How+SolrCloud+Works [2] https://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud Note Mark's comment on that JIRA, but I think it would be best to continue to talk about "shards" with user-facing operations.</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.handler.admin.CollectionsHandler.java</file><file>solr.core.src.java.org.apache.solr.cloud.Overseer.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestCollectionAPI.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestReplicaProperties.java</file><file>solr.solrj.src.java.org.apache.solr.common.params.CollectionParams.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file></fixedFiles></bug><bug fixdate="2015-01-05 10:31:59" id="6666" opendate="2014-10-29 09:08:04"><buginformation><summary>[SOLR-6666] Dynamic copy fields are considering all dynamic fields, causing a significant performance impact on indexing documents - ASF JIRA</summary><description>Result: After applying a fix for this issue, tests which we conducted show more than 40 percent improvement on our insertion performance. Explanation: Using JVM profiler, we found a CPU "bottleneck" during Solr indexing process. This bottleneck can be found at org.apache.solr.schema.IndexSchema, in the following method, "getCopyFieldsList()": getCopyFieldsList() &#13;
final List&lt;CopyField&gt; result = new ArrayList&lt;&gt;();&#13;
    for (DynamicCopy dynamicCopy : dynamicCopyFields) {&#13;
      if (dynamicCopy.matches(sourceField)) {&#13;
        result.add(new CopyField(getField(sourceField), dynamicCopy.getTargetField(sourceField), dynamicCopy.maxChars));&#13;
      }&#13;
    }&#13;
    List&lt;CopyField&gt; fixedCopyFields = copyFieldsMap.get(sourceField);&#13;
    if (null != fixedCopyFields) {&#13;
      result.addAll(fixedCopyFields);&#13;
    }&#13;
 This function tries to find for an input source field all its copyFields (All its destinations which Solr need to move this field). As you can probably note, the first part of the procedure is the procedure most “expensive” step (takes O( n ) time while N is the size of the "dynamicCopyFields" group). The next part is just a simple "hash" extraction, which takes O(1) time. Our schema contains over then 500 copyFields but only 70 of then are "indexed" fields. We also have one dynamic field with a wildcard ( * ), which "catches" the rest of the document fields. As you can conclude, we have more than 400 copyFields that are based on this dynamicField but all, except one, are fixed (i.e. does not contain any wildcard). From some reason, the copyFields registration procedure defines those 400 fields as "DynamicCopyField " and then store them in the “dynamicCopyFields” array, This step makes getCopyFieldsList() very expensive (in CPU terms) without any justification: All of those 400 copyFields are not glob and therefore do not need any complex pattern matching to the input field. They all can be store at the "fixedCopyFields". Only copyFields with asterisks need this "special" treatment and they are (especially on our case) pretty rare. Therefore, we created a patch which fix this problem by changing the registerCopyField() procedure. Test which we conducted show that there is no change in the Indexing results. Moreover, the fix still successfully passes the class unit tests (i.e. IndexSchemaTest.java).</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.rest.schema.TestSchemaResource.java</file><file>solr.core.src.java.org.apache.solr.schema.IndexSchema.java</file><file>solr.core.src.test.org.apache.solr.rest.schema.TestCopyFieldCollectionResource.java</file></fixedFiles></bug><bug fixdate="2014-10-28 02:53:35" id="6655" opendate="2014-10-26 09:09:29"><buginformation><summary>[SOLR-6655] Improve SimplePostTool to easily specify target port/collection etc. - ASF JIRA</summary><description>Right now, the SimplePostTool has a single parameter 'url' that can be used to send the request to a specific endpoint. It would make sense to allow users to specify just the collection name, port etc. explicitly and independently as separate parameters.</description></buginformation><fixedFiles><file>core.src.java.org.apache.solr.util.SimplePostTool.java</file></fixedFiles></bug><bug fixdate="2014-10-14 08:34:08" id="6617" opendate="2014-10-10 10:53:29"><buginformation><summary>[SOLR-6617] /update/json/docs handler needs to do a better job with tweet like JSON structures - ASF JIRA</summary><description>SOLR-6304 allows me to send in arbitrary JSON document and have Solr do something reasonable with it. I tried this with a simple tweet and got a weird error: &#13;
curl "http://localhost:8983/solr/tutorial/update/json/docs" -H 'Content-type:application/json' -d @sample_tweet.json&#13;
&#13;
{"responseHeader":{"status":400,"QTime":11},"error":{"msg":"Document contains multiple values for uniqueKey field: id=[14065694, 136447843652214784]","code":400}}&#13;
 Here's the tweet I'm trying to index: &#13;
{&#13;
        "user": {&#13;
            "name": "John Doe",&#13;
            "screen_name": "example",&#13;
            "lang": "en",&#13;
            "time_zone": "London",&#13;
            "listed_count": 221,&#13;
            "id": 14065694,&#13;
            "geo_enabled": true&#13;
        },&#13;
        "id": "136447843652214784",&#13;
        "text": "Morning San Francisco - 36 hours and counting.. #datasift",&#13;
        "created_at": "Tue, 15 Nov 2011 14:17:55 +0000"&#13;
}&#13;
 The error is because the nested user object within the tweet also has an "id" field. So then I tried to map /user/id to user_id_s via: &#13;
curl "http://localhost:8983/solr/tutorial/update/json/docs?f=user_id_s:/user/id" -H 'Content-type:application/json' -d @sample_tweet.json&#13;
{"responseHeader":{"status":400,"QTime":0},"error":{"msg":"Document is missing mandatory uniqueKey field: id","code":400}}&#13;
 So then I added the mapping for id explicitly and it worked: curl "http://localhost:8983/solr/tutorial/update/json/docs?f=id:/id&amp;f=user_id_s:/user/id" -H 'Content-type:application/json' -d @sample_tweet.json {"responseHeader":{"status":0,"QTime":25}} Working through this wasn't terrible but our goal with features like this is to have Solr make good decisions when possible to ease the new user's burden of getting to know Solr. I'm just wondering if the reasonable thing to do wouldn't be to map the user fields with user_ prefix? ie /user/id becomes user_id automatically. Lastly, I wanted to use field guessing with this so my JSON document gets indexed in a reasonable way and the only data that got indexed is: &#13;
{&#13;
        "user_id_s": "14065694",&#13;
        "id": "136447843652214784",&#13;
        "_version_": 1481614081193410600&#13;
}&#13;
 So I explicitly defined the /update/json/docs request handler in my solrconfig.xml as: &#13;
  &lt;requestHandler name="/update/json/docs" class="solr.UpdateRequestHandler"&gt;&#13;
        &lt;lst name="defaults"&gt;&#13;
         &lt;str name="update.chain"&gt;add-unknown-fields-to-the-schema&lt;/str&gt;&#13;
         &lt;str name="stream.contentType"&gt;application/json&lt;/str&gt;&#13;
       &lt;/lst&gt;&#13;
  &lt;/requestHandler&gt;&#13;
 Same result - no field guessing! (this is using the schemaless example config)</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.common.util.JsonRecordReader.java</file><file>solr.solrj.src.test.org.apache.solr.common.util.TestJsonRecordReader.java</file><file>solr.core.src.java.org.apache.solr.handler.loader.JsonLoader.java</file></fixedFiles></bug><bug fixdate="2014-10-31 05:02:37" id="6597" opendate="2014-10-06 09:11:00"><buginformation><summary>[SOLR-6597] SolrIndexConfig parameter in SolrIndexSearcher constructor is not used - ASF JIRA</summary><description>The following constructor of SolrIndexSearcher doesn't use 'config'. SolrIndexSearcher &#13;
public SolrIndexSearcher(SolrCore core, String path, IndexSchema schema, SolrIndexConfig config, String name, DirectoryReader r, boolean closeReader, boolean enableCache, boolean reserveDirectory, DirectoryFactory directoryFactory)&#13;
 It doesn't make sense to pass in the SolrIndexConfig when we're passing in the DirectoryReader (and asserting that it's never null). Prior to LUCENE-5666, when 'r' was null, the config was used to get a reader but not any more. I'll just remove the param from the constructor and remove it from all places that calls this version of the constructor.</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.search.SolrIndexSearcher.java</file><file>solr.core.src.java.org.apache.solr.core.SolrCore.java</file></fixedFiles></bug><bug fixdate="2014-09-26 11:05:52" id="6565" opendate="2014-09-25 11:33:59"><buginformation><summary>[SOLR-6565] SolrRequest support for query params - ASF JIRA</summary><description>Today, queryParams (sending params via the queryString) is only supported at the HttpSolrServer level. In an effort to make SolrRequests more generally useful (a la SOLR-6543), it would be nice if you could set the queryParams on a per-request basis.</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.client.solrj.SolrRequest.java</file><file>solr.solrj.src.java.org.apache.solr.client.solrj.impl.HttpSolrServer.java</file><file>solr.solrj.src.test.org.apache.solr.client.solrj.impl.BasicHttpSolrServerTest.java</file></fixedFiles></bug><bug fixdate="2014-12-18 05:45:23" id="6554" opendate="2014-09-23 06:55:38"><buginformation><summary>[SOLR-6554] Speed up overseer operations for collections with stateFormat &gt; 1 - ASF JIRA</summary><description>Right now (after SOLR-5473 was committed), a node watches a collection only if stateFormat=1 or if that node hosts at least one core belonging to that collection. This means that a node which is the overseer operates on all collections but watches only a few. So any read goes directly to zookeeper which slows down overseer operations. Let's have the overseer node watch all collections always and never remove those watches (except when the collection itself is deleted).</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.cloud.overseer.OverseerAction.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.ZkWriteCommand.java</file><file>solr.core.src.test.org.apache.solr.cloud.overseer.TestClusterStateMutator.java</file><file>solr.core.src.test.org.apache.solr.cloud.CollectionsAPIDistributedZkTest.java</file><file>solr.solrj.src.java.org.apache.solr.common.cloud.DocCollection.java</file><file>solr.solrj.src.java.org.apache.solr.common.cloud.ZkStateReader.java</file><file>solr.core.src.java.org.apache.solr.update.processor.DistributedUpdateProcessor.java</file><file>solr.solrj.src.java.org.apache.solr.common.cloud.ClusterState.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.CollectionMutator.java</file><file>solr.core.src.java.org.apache.solr.handler.admin.CollectionsHandler.java</file><file>solr.core.src.java.org.apache.solr.cloud.Overseer.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.ReplicaMutator.java</file><file>solr.core.src.test.org.apache.solr.cloud.overseer.ZkStateWriterTest.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.SliceMutator.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file><file>solr.core.src.java.org.apache.solr.cloud.ElectionContext.java</file><file>solr.core.src.test.org.apache.solr.cloud.ShardSplitTest.java</file><file>solr.core.src.java.org.apache.solr.cloud.ZkController.java</file><file>solr.core.src.test.org.apache.solr.cloud.OverseerTest.java</file><file>solr.core.src.test.org.apache.solr.cloud.OverseerRolesTest.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.ClusterStateMutator.java</file><file>solr.core.src.java.org.apache.solr.cloud.overseer.ZkStateWriter.java</file><file>solr.core.src.test.org.apache.solr.cloud.DeleteShardTest.java</file></fixedFiles></bug><bug fixdate="2014-10-02 04:51:57" id="6550" opendate="2014-09-22 08:37:25"><buginformation><summary>[SOLR-6550] Provide simple mechanism for passing additional metadata / context about a server-side SolrException back to the client-side - ASF JIRA</summary><description>While trying to resolve SOLR-6511, it became apparent that I didn't have a good way to convey more information about a particular error occurring on the server-side using SolrException. The specific situation I encountered is a replica took over as leader, but the previous leader wasn't aware of that yet (due to a Zk session expiration). So when the previous leader (the one that experienced the Zk session expiration) sent an update request with FROMLEADER, the new leader rejected the request with a SolrException. Ideally, we want the new leader to be able to say "you're not the leader anymore" and for the previous leader to fail the request in a specific way; see SOLR-6511 for more background on this scenario. My first inclination was to just extend SolrException and throw a LeaderChangedException and have the client behave accordingly but then I discovered that CUSS just takes the status code and error message and reconstructs a new SolrException (on the client side). HttpSolrServer does the same thing when creating a RemoteSolrException. So the fact that the server-side throw a LeaderChangeException is basically lost in translation. I'm open to other suggestions but here's my approach so far: Add a NamedList&lt;String&gt; metadata field to the SolrException class. If a server-side component wants to add additional context / metadata, then it will call: solrExc.setMetadata("name", "value); When the response is being marshaled into the wire format, ResponseUtils will include the metadata if available. On the client side, when the response is processed, the metadata gets included into the new SolrException (in CUSS) or RemoteSolrException (HttpSolrServer). It's up to the client to dig into the metadata to take additional steps as I'll be doing in DistributedUpdateProcessor.</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.client.solrj.impl.HttpSolrServer.java</file><file>solr.solrj.src.java.org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.java</file><file>solr.core.src.java.org.apache.solr.servlet.ResponseUtils.java</file><file>solr.solrj.src.java.org.apache.solr.common.SolrException.java</file></fixedFiles></bug><bug fixdate="2014-10-31 05:04:40" id="6544" opendate="2014-09-19 09:53:54"><buginformation><summary>[SOLR-6544] Remove unused arguments from methods in DeleteReplicaTest - ASF JIRA</summary><description>There are unused arguments being passed in helper methods in DeleteReplicaTest. We should remove those to avoid confusion. Also, the test has some unwanted iterations to get an active replica, fix that.</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.cloud.DeleteReplicaTest.java</file></fixedFiles></bug><bug fixdate="2014-09-25 07:25:50" id="6543" opendate="2014-09-19 08:19:24"><buginformation><summary>[SOLR-6543] Give HttpSolrServer the ability to send PUT requests - ASF JIRA</summary><description>Given that the schema API has a PUT request (https://cwiki.apache.org/confluence/display/solr/Schema+API#SchemaAPI-Createonenewschemafield) it would be nice if HttpSolrServer supported sending PUTs, so it could be used for sending that type of request. Note if we really wanted to fully support that request we'd probably want a Request/Response type in solrj as well, but that can be handled in a separate issue. Also, administrators may add arbitrary filters that require PUT requests. In my own setup, I have a version of Hadoop's DelegationTokenAuthenticationFilter sitting in front of the dispatch filter. Here also it would be nice if I could send all requests via HttpSolrServer.</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.client.solrj.SolrRequest.java</file><file>solr.solrj.src.java.org.apache.solr.client.solrj.impl.HttpSolrServer.java</file><file>solr.test-framework.src.java.org.apache.solr.util.RestTestHarness.java</file><file>solr.solrj.src.test.org.apache.solr.client.solrj.impl.BasicHttpSolrServerTest.java</file></fixedFiles></bug><bug fixdate="2014-12-18 11:52:29" id="6523" opendate="2014-09-16 09:45:26"><buginformation><summary>[SOLR-6523] Provide SolrJ support for specifying stateFormat while creating Collections - ASF JIRA</summary><description>The new zk state format (as introduced by SOLR-5473) should be supported by CollectionsAdminRequest/SolrJ Create() class.</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.client.solrj.request.CollectionAdminRequest.java</file><file>solr.core.src.test.org.apache.solr.cloud.CollectionsAPIDistributedZkTest.java</file></fixedFiles></bug><bug fixdate="2015-01-22 02:47:09" id="6521" opendate="2014-09-15 09:40:45"><buginformation><summary>[SOLR-6521] CloudSolrClient should synchronize cache cluster state loading - ASF JIRA</summary><description>Under heavy load-testing with the new solrj client that caches the cluster state instead of setting a watcher, I started seeing lots of zk connection loss on the client-side when refreshing the CloudSolrServer collectionStateCache, and this was causing crazy client-side 99.9% latency (~15 sec). I swapped the cache out with guava's LoadingCache (which does locking to ensure only one thread loads the content under one key while the other threads that want the same key wait) and the connection loss went away and the 99.9% latency also went down to just about 1 sec.</description></buginformation><fixedFiles><file>solr.solrj.src.java.org.apache.solr.common.cloud.ClusterState.java</file><file>solr.solrj.src.java.org.apache.solr.client.solrj.impl.CloudSolrClient.java</file><file>solr.solrj.src.java.org.apache.solr.common.cloud.ZkStateReader.java</file></fixedFiles></bug><bug fixdate="2014-10-08 07:14:31" id="6513" opendate="2014-09-12 07:55:42"><buginformation><summary>[SOLR-6513] Add a collectionsAPI call BALANCESLICEUNIQUE - ASF JIRA</summary><description>Another sub-task for SOLR-6491. The ability to assign a property on a node-by-node basis is nice, but tedious to get right for a sysadmin, especially if there are, say, 100s of nodes hosting a system. This JIRA would essentially provide an automatic mechanism for assigning a property. This particular command simply changes the cluster state, it doesn't do anything like re-assign functions. My idea for this version is fairly limited. You'd have to specify a collection and there would be no attempt to, say, evenly distribute the preferred leader role/property for this collection by looking at other collections. Or by looking at underlying hardware capabilities. Or.... It would be a pretty simple round-robin assignment. About the only intelligence built in would be to change as few roles/properties as possible. Let's say that the correct number of nodes for this role turned out to be 3. Any node currently having 3 properties for this collection would NOT be changed. Any node having 2 properties would have one added that would be taken from some node with &gt; 3 properties like this. This probably needs an optional parameter, something like "includeInactiveNodes=true|false" Since this is an arbitrary property, one must specify sliceUnique=true. So for the "preferredLeader" functionality, one would specify something like: action=BALANCESLICEUNIQUE&amp;property=preferredLeader&amp;proprety.value=true. There are checks in this code that require the preferredLeader to have a t/f value and require that sliceUnique bet true. That said, this can be called on an arbitrary property that has only one such property per slice.</description></buginformation><fixedFiles><file>solr.core.src.java.org.apache.solr.handler.admin.CollectionsHandler.java</file><file>solr.core.src.java.org.apache.solr.cloud.Overseer.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestReplicaRoles.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestCollectionAPI.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestReplicaProperties.java</file><file>solr.solrj.src.java.org.apache.solr.common.params.CollectionParams.java</file><file>solr.core.src.test.org.apache.solr.cloud.ReplicaPropertiesBase.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file></fixedFiles></bug><bug fixdate="2014-10-01 07:27:19" id="6512" opendate="2014-09-12 07:42:43"><buginformation><summary>[SOLR-6512] Add a collections API call to add/delete arbitrary properties to a specific replica - ASF JIRA</summary><description>This is a sub-task for SOLR-6491, but seems generally useful. Since this is in support of the "preferredLeader" functionality, I've run into some considerations that I wanted some feedback on how to handle. "preferredLeader" has the restriction that there should only be one per slice, so setting this for a particular node means removing the property for all the other replicas on the slice. Not a problem to do, my question is more whether this is something reasonable to enforce on an arbitrary property based on what that property is? Perfectly do-able, but "semantically challenged". Currently, this is never a node with "preferedLeader" set to "false", it is forcibly removed from other nodes in the slice when this property is assigned. The problem here is that there's nothing about assigning an arbitrary property to a node that would reasonably imply this kind of behavior. One could always control this with secondary flags on the command, e.g. "shardExclusive=true|false" for instance, perhaps with safety checks in for known one-per-shard properties like "preferredLeader". "preferredLeader" seems to fit more naturally into a "role", but currently ADDROLE and DELTEROLE have nothing to do with the notion of setting a role for a particular node relative to a collection/shard. Easy enough to add, but enforcing the "only one node per slice may have this role" rule there is similarly arbitrary and overloads the ADDROLE/DELETEROLE in a way that seems equally confusing. Plus, checking whether the required collection/shard/node params are present becomes based on the value of the property being set, which is all equally arbitrary. The other interesting thing is that setting an arbitrary property on a node would allow one to mess things up royally by, say, changing properties like "core", or "base_url" or node_name at will. Actually this is potentially useful, but very, very dangerous and I'm not particularly interested in supporting it . I suppose we could require a prefix, say the only settable properties are "property.whatever". We could also add something specific to nodes, something like ADDREPLICAROLE/DELETEREPLICAROLE, perhaps with sub-params like "onlyOneAllowedPerShard", but this gets messy and relies on the users "doing the right thing". I prefer enforcing rules like this based on the role I think. Or at least enforcing these kinds of requirements on the "preferredLeader" role if we go that way. What are people's thoughts here? I think I'm tending towards the ADDREPLICAROLE/DELETEREPLICAROLE way of doing this, but it's not set in stone. I have code locally for arbitrary properties that I can modify for the role bits. So, if I'm going to summarize the points I'd like feedback on: 1&gt; Is setting arbitrary properties on a node desirable? If so, should we require a prefix like "property" to prevent resetting values SolrCloud depends on? 2&gt; Is it better to piggyback on ADDROLE/DELETEROLE? Personally I'm not in favor of this one. Too messy with requiring additional parameters to work right in this case 3&gt; Is the best option to create new collections API calls for ADDREPLICAROLE/DELETEREPLICAROLE that 3.1&gt; require collection/slice/node parameters 3.2&gt; enforces the "onlyOnePerShard" rule for certain known roles 3.3 v1&gt; allows users to specify arbitrary roles something like "onlyOnePerShard" as an optional T|F parameter, otherwise is totally open. or 3.3 v2&gt; No support other than "preferredLeader", only roles that are pre-defined are allowed, in which case the "onlyOnePerShard" is implicit in the role.</description></buginformation><fixedFiles><file>solr.core.src.test.org.apache.solr.cloud.DeleteReplicaTest.java</file><file>solr.core.src.java.org.apache.solr.handler.admin.CollectionsHandler.java</file><file>solr.core.src.java.org.apache.solr.cloud.Overseer.java</file><file>solr.core.src.test.org.apache.solr.cloud.TestCollectionAPI.java</file><file>solr.solrj.src.java.org.apache.solr.common.cloud.ZkStateReader.java</file><file>solr.solrj.src.java.org.apache.solr.common.params.CollectionParams.java</file><file>solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor.java</file></fixedFiles></bug><bug fixdate="2014-09-07 09:55:13" id="6488" opendate="2014-09-06 09:34:12"><buginformation><summary>[SOLR-6488] Upgrade to TIKA 1.6 - ASF JIRA</summary><description>Apache TIKA 1.6 came out yesterday, we should upgrade it. The dependencies of bundled Apache POI changed (xmlbeans upgraded, already done. dom4j is obsolete). We have to carefully verify the dependency tree!!!</description></buginformation><fixedFiles><file>solr.contrib.extraction.src.test.org.apache.solr.handler.extraction.ExtractingRequestHandlerTest.java</file></fixedFiles></bug></bugrepository>