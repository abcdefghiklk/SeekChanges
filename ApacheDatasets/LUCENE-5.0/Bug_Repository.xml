<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository><bug fixdate="2015-02-27 10:23:49" id="6299" opendate="2015-02-25 11:35:04"><buginformation><summary>[LUCENE-6299] IndexWriter's enforcement of 2.1B doc limits is buggy - ASF JIRA</summary><description>E.g. if you pass an already &gt; 2.1B docs to either addIndexes, it can fail to enforce properly. IW's private reserveDocs should refuse to accept negative values. IW.deleteAll fails to set the pendingNumDocs to 0.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.index.TestDemoParallelLeafReader.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.MockDirectoryWrapper.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.RawDirectoryWrapper.java</file><file>lucene.core.src.java.org.apache.lucene.index.StandardDirectoryReader.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestSearcherManager.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseStoredFieldsFormatTestCase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.search.QueryUtils.java</file><file>solr.core.src.java.org.apache.solr.search.SolrIndexSearcher.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.MismatchedDirectoryReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.BaseCompositeReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.ExitableDirectoryReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.highlighter.src.java.org.apache.lucene.search.postingshighlight.PostingsHighlighter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.AssertingDirectoryReader.java</file><file>lucene.core.src.test.org.apache.lucene.store.TestMockDirectoryWrapper.java</file><file>lucene.core.src.java.org.apache.lucene.index.FilterDirectoryReader.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriterMaxDocs.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriterPerThread.java</file><file>lucene.core.src.java.org.apache.lucene.index.MultiReader.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.BaseDirectoryWrapper.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.LuceneTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.index.DirectoryReader.java</file><file>lucene.misc.src.java.org.apache.lucene.uninverting.UninvertingReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentInfos.java</file></fixedFiles></bug><bug fixdate="2015-02-25 10:10:02" id="6214" opendate="2015-01-31 10:10:01"><buginformation><summary>[LUCENE-6214] IW deadlocks if commit and reopen happens concurrently while exception is hit - ASF JIRA</summary><description>I just hit this while working on an elasticseach test using a lucene 5.1 snapshot (5.1.0-snapshot-1654549). The test throws random exceptions via MockDirWrapper and deadlocks, jstack says: Found one Java-level deadlock:&#13;
=============================&#13;
"elasticsearch[node_2][refresh][T#2]":&#13;
  waiting to lock monitor 0x00007fe51314c098 (object 0x00000007018ee8d8, a java.lang.Object),&#13;
  which is held by "elasticsearch[node_2][generic][T#1]"&#13;
"elasticsearch[node_2][generic][T#1]":&#13;
  waiting to lock monitor 0x00007fe512d74b68 (object 0x00000007018ee8e8, a java.lang.Object),&#13;
  which is held by "elasticsearch[node_2][refresh][T#2]"&#13;
&#13;
Java stack information for the threads listed above:&#13;
===================================================&#13;
"elasticsearch[node_2][refresh][T#2]":&#13;
	at org.apache.lucene.index.IndexWriter.tragicEvent(IndexWriter.java:4441)&#13;
	- waiting to lock &lt;0x00000007018ee8d8&gt; (a java.lang.Object)&#13;
	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:436)&#13;
	- locked &lt;0x00000007018ee8e8&gt; (a java.lang.Object)&#13;
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:281)&#13;
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:256)&#13;
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:246)&#13;
	at org.apache.lucene.index.FilterDirectoryReader.doOpenIfChanged(FilterDirectoryReader.java:104)&#13;
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:123)&#13;
	at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:137)&#13;
	at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)&#13;
	at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176)&#13;
	at org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:703)&#13;
	at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:500)&#13;
	at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:954)&#13;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&#13;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
"elasticsearch[node_2][generic][T#1]":&#13;
	at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2730)&#13;
	- waiting to lock &lt;0x00000007018ee8e8&gt; (a java.lang.Object)&#13;
	- locked &lt;0x00000007018ee8d8&gt; (a java.lang.Object)&#13;
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2888)&#13;
	- locked &lt;0x00000007018ee8d8&gt; (a java.lang.Object)&#13;
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2855)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine.commitIndexWriter(InternalEngine.java:722)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:800)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.endRecovery(InternalEngine.java:1520)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.close(InternalEngine.java:1533)&#13;
	at org.elasticsearch.common.lease.Releasables.close(Releasables.java:45)&#13;
	at org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:70)&#13;
	at org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:75)&#13;
	at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1048)&#13;
	at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:635)&#13;
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:120)&#13;
	at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:48)&#13;
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:141)&#13;
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:127)&#13;
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:287)&#13;
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)&#13;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&#13;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&#13;
	at java.lang.Thread.run(Thread.java:745)&#13;
&#13;
Found 1 deadlock.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestTragicIndexWriterDeadlock.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriterTragicDeadlock.java</file></fixedFiles></bug><bug fixdate="2015-01-29 04:04:55" id="6207" opendate="2015-01-28 08:27:09"><buginformation><summary>[LUCENE-6207] Multiple filtered subsets of the same underlying index passed to IW.addIndexes() can produce an index with bad SortedDocValues - ASF JIRA</summary><description>Were hit by this in a custom index splitter implementation that showed no problems with Lucene 4.8. After upgrading to 4.10 documents started having wrong SortedDocValues after splitting.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.codecs.lucene50.TestLucene50DocValuesFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseDocValuesFormatTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.index.SortedDocValuesTermsEnum.java</file><file>lucene.core.src.java.org.apache.lucene.index.SortedSetDocValuesTermsEnum.java</file></fixedFiles></bug><bug fixdate="2015-02-25 08:58:00" id="6205" opendate="2015-01-28 05:39:46"><buginformation><summary>[LUCENE-6205] DV updates can hit FileNotFoundException due to concurrency bug - ASF JIRA</summary><description>Jenkins has hit this a few times recently, e.g.:  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates&#13;
   [junit4]   2&gt; Jan 28, 2015 11:49:24 AM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException&#13;
   [junit4]   2&gt; WARNUNG: Uncaught exception in thread: Thread[Lucene Merge Thread #1,5,TGRP-TestBinaryDocValuesUpdates]&#13;
   [junit4]   2&gt; org.apache.lucene.index.MergePolicy$MergeException: java.nio.file.NoSuchFileException: _4_1.fnm in dir=RAMDirectory@5dcf7f8a lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@ccb4148&#13;
   [junit4]   2&gt; 	at __randomizedtesting.SeedInfo.seed([5EC20FA2CD1E68B8]:0)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:641)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:609)&#13;
   [junit4]   2&gt; Caused by: java.nio.file.NoSuchFileException: _4_1.fnm in dir=RAMDirectory@5dcf7f8a lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@ccb4148&#13;
   [junit4]   2&gt; 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:655)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:110)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat.read(Lucene50FieldInfosFormat.java:113)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.SegmentReader.initFieldInfos(SegmentReader.java:155)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:119)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3935)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3559)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:549)&#13;
   [junit4]   2&gt; 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:587)&#13;
   [junit4]   2&gt; &#13;
   [junit4]   2&gt; NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=5EC20FA2CD1E68B8 -Dtests.slow=true -Dtests.locale=de_DE -Dtests.timezone=Europe/Samara -Dtests.asserts=true -Dtests.file.encoding=UTF-8&#13;
 It repros only after substantial beasting. It's a concurrency issue between one thread kicking off a merge, and another thread resolving doc values updates.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file></fixedFiles></bug><bug fixdate="2015-01-24 03:18:39" id="6197" opendate="2015-01-23 04:57:08"><buginformation><summary>[LUCENE-6197] ConcurrentMergeScheduler should not stall its own merge threads! - ASF JIRA</summary><description>http://build-eu-00.elasticsearch.org/job/lucene_trunk_linux_java8_64_analyzers/25834 uncovered this issue. I accidentally introduced this with auto-IO-throttle ( LUCENE-6119) ... the CMS.maybeStall method, which is supposed to block "segment producing" threads so indexing slows down when merges cannot keep up, can now sometimes block its own merge threads. This happens when the merge thread re-invokes CMS.merge after it finishes, so new merges can kick off. This is really silly, since merge threads are not segment producers, but rather the "messengers", spawned by the true segment producers, so CMS should not shoot the messenger here. I think it's also possible this could lead to deadlock in CMS, but I'm not certain, and I couldn't provoke it. I'd like to fix this for 5.0 because it was first introduced in 5.0 and not yet released...</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.ConcurrentMergeScheduler.java</file><file>lucene.core.src.java.org.apache.lucene.index.MergeTrigger.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestConcurrentMergeScheduler.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.LuceneTestCase.java</file></fixedFiles></bug><bug fixdate="2015-01-21 04:53:52" id="6192" opendate="2015-01-20 09:28:20"><buginformation><summary>[LUCENE-6192] Long overflow in LuceneXXSkipWriter can corrupt skip data - ASF JIRA</summary><description>I've been iterating with Tom on this corruption that CheckIndex detects in his rather large index (720 GB in a single segment):  java -Xmx16G -Xms16G -cp $JAR -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /XXXX/shards/4/core-1/data/test_index -verbose 2&gt;&amp;1 |tee -a shard4_reoptimizedNewJava&#13;
&#13;
&#13;
Opening index @ /htsolr/lss-reindex/shards/4/core-1/data/test_index&#13;
&#13;
Segments file=segments_e numSegments=1 version=4.10.2 format= userData={commitTimeMSec=1421479358825}&#13;
  1 of 1: name=_8m8 docCount=1130856&#13;
    version=4.10.2&#13;
    codec=Lucene410&#13;
    compound=false&#13;
    numFiles=10&#13;
    size (MB)=719,967.32&#13;
    diagnostics = {timestamp=1421437320935, os=Linux, os.version=2.6.18-400.1.1.el5, mergeFactor=2, source=merge, lucene.version=4.10.2, os.arch=amd64, mergeMaxNumSegments=1, java.version=1.7.0_71, java.vendor=Oracle Corporation}&#13;
    no deletions&#13;
    test: open reader.........OK&#13;
    test: check integrity.....OK&#13;
    test: check live docs.....OK&#13;
    test: fields..............OK [80 fields]&#13;
    test: field norms.........OK [23 fields]&#13;
    test: terms, freq, prox...ERROR: java.lang.AssertionError: -96&#13;
java.lang.AssertionError: -96&#13;
        at org.apache.lucene.codecs.lucene41.ForUtil.skipBlock(ForUtil.java:228)&#13;
        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader$BlockDocsAndPositionsEnum.skipPositions(Lucene41PostingsReader.java:925)&#13;
        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader$BlockDocsAndPositionsEnum.nextPosition(Lucene41PostingsReader.java:955)&#13;
        at org.apache.lucene.index.CheckIndex.checkFields(CheckIndex.java:1100)&#13;
        at org.apache.lucene.index.CheckIndex.testPostings(CheckIndex.java:1357)&#13;
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:655)&#13;
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)&#13;
    test: stored fields.......OK [67472796 total field count; avg 59.665 fields per doc]&#13;
    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]&#13;
    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]&#13;
FAILED&#13;
    WARNING: fixIndex() would remove reference to this segment; full exception:&#13;
java.lang.RuntimeException: Term Index test failed&#13;
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:670)&#13;
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)&#13;
&#13;
WARNING: 1 broken segments (containing 1130856 documents) detected&#13;
WARNING: would write new segments file, and 1130856 documents would be lost, if -fix were specified&#13;
 And Rob spotted long -&gt; int casts in our skip list writers that look like they could cause such corruption if a single high-freq term with many positions required &gt; 2.1 GB to write its positions into .pos.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.codecs.lucene41.Lucene41SkipWriter.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.lucene41.Lucene41SkipReader.java</file></fixedFiles></bug><bug fixdate="2015-01-08 03:21:32" id="6165" opendate="2015-01-07 08:40:55"><buginformation><summary>[LUCENE-6165] Change merging APIs to work on CodecReader instead of LeafReader - ASF JIRA</summary><description>Patch factors out "reader based on codec apis" and changes all merge policy/addIndexes apis to use this. If you want to do slow wrapping, you can still do it, just use SlowCodecReaderWrapper.wrap(LeafReader) yourself (versus SegmentMerger doing it always if its not a SegmentReader). Also adds FilterCodecReader, to make it easier to start efficiently filtering on merge. I cutover all the index splitters to this. This means they should be much much faster with this patch, they just change the deletes as you expect, and the merge is as optimal as a normal one. In other places, for now I think we should just do a rote conversion with SlowCodecReaderWrapper.wrap. Its no slower than today, just explicit, and we can incrementally fix them to do the right thing in the future rather than all at once.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.MergeState.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDemoParallelLeafReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.FilterCodecReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.MergePolicy.java</file><file>lucene.core.src.java.org.apache.lucene.index.SlowCodecReaderWrapper.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestFilterLeafReader.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.MockRandomMergePolicy.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentReader.java</file><file>lucene.misc.src.test.org.apache.lucene.index.TestMultiPassIndexSplitter.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentMerger.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestParallelReaderEmptyIndex.java</file><file>solr.core.src.java.org.apache.solr.update.SolrIndexSplitter.java</file><file>lucene.facet.src.java.org.apache.lucene.facet.taxonomy.TaxonomyMergeUtils.java</file><file>lucene.misc.src.java.org.apache.lucene.index.MultiPassIndexSplitter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestAddIndexes.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.RandomIndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDoc.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestSegmentMerger.java</file><file>lucene.misc.src.java.org.apache.lucene.index.PKIndexSplitter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.TestUtil.java</file><file>lucene.misc.src.test.org.apache.lucene.index.IndexSortingTest.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDocValuesIndexing.java</file><file>solr.core.src.java.org.apache.solr.update.DirectUpdateHandler2.java</file><file>lucene.core.src.java.org.apache.lucene.index.TrackingIndexWriter.java</file><file>lucene.core.src.java.org.apache.lucene.index.CodecReader.java</file><file>lucene.benchmark.src.java.org.apache.lucene.benchmark.byTask.tasks.AddIndexesTask.java</file><file>lucene.misc.src.java.org.apache.lucene.index.MergeReaderWrapper.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.misc.src.java.org.apache.lucene.index.SortingMergePolicy.java</file></fixedFiles></bug><bug fixdate="2015-01-05 02:38:53" id="6159" opendate="2015-01-03 10:55:17"><buginformation><summary>[LUCENE-6159] TestSearcherManager sometimes uses too many files - ASF JIRA</summary><description>on branch_5x: ant test -Dtestcase=TestSearcherManager -Dtests.seed=D6BC19E58A39CA7 -Dtests.multiplier=2 -Dtests.nightly=true it reproduces, its not hitting the operating system limit, instead the mockfilesystem one in TestRuleTemporaryFilesCleanup: &#13;
  // os/config-independent limit for too many open files&#13;
  // TODO: can we make this lower?&#13;
  private static final int MAX_OPEN_FILES = 2048;&#13;
 I havent looked further only to check it reproduced.</description></buginformation><fixedFiles><file>lucene.test-framework.src.java.org.apache.lucene.index.ThreadedIndexingAndSearchingTestCase.java</file></fixedFiles></bug><bug fixdate="2015-01-07 03:15:53" id="6158" opendate="2015-01-03 05:18:28"><buginformation><summary>[LUCENE-6158] IW.addIndexes(IndexReader...) -&gt; IW.addIndexes(LeafReader...) - ASF JIRA</summary><description>addIndexes(IndexReader...) is useful to force a single merge that transforms data: you wrap the readers with some logic that alters them. But for any use case doing this, they need to work on leaves (LeafReader) to actually do anything. Otherwise, for simply merging indexes, allowing addIndexes(IndexReader) is unnecessary and maybe a slight trap, its way faster to call addIndexes(Directory), and it won't force a single slow merge, but will just copy in the relevant files and call maybeMerge(). Part of the confusion is the two methods have such different behavior that i don't think they should be both be named addIndexes. But lets do that separately, first i want to fix the parameters. Long term taking LeafReader here is a simple step towards a more performant api for "merging with filterreader", since its horribly inefficient today.</description></buginformation><fixedFiles><file>lucene.misc.src.test.org.apache.lucene.index.IndexSortingTest.java</file><file>solr.core.src.java.org.apache.solr.update.DirectUpdateHandler2.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseStoredFieldsFormatTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.index.TrackingIndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestTermVectors.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestTermVectors.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestParallelReaderEmptyIndex.java</file><file>lucene.benchmark.src.java.org.apache.lucene.benchmark.byTask.tasks.AddIndexesTask.java</file><file>lucene.core.src.test.org.apache.lucene.index.Test2BPostingsBytes.java</file><file>lucene.backward-codecs.src.test.org.apache.lucene.index.TestBackwardsCompatibility.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>solr.core.src.java.org.apache.solr.update.SolrIndexSplitter.java</file><file>lucene.facet.src.java.org.apache.lucene.facet.taxonomy.TaxonomyMergeUtils.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestNumericDocValuesUpdates.java</file><file>lucene.misc.src.java.org.apache.lucene.index.MultiPassIndexSplitter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestAddIndexes.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseIndexFileFormatTestCase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.RandomIndexWriter.java</file><file>lucene.misc.src.java.org.apache.lucene.index.PKIndexSplitter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.TestUtil.java</file></fixedFiles></bug><bug fixdate="2014-12-31 03:53:47" id="6152" opendate="2014-12-31 02:24:01"><buginformation><summary>[LUCENE-6152] Fix double close bug in OutputStreamIndexOutput - ASF JIRA</summary><description>As discovered in LUCENE-6151, we shouldprevent calling flush() twice when closing OutputStreamIndexOutput two times.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.store.OutputStreamIndexOutput.java</file></fixedFiles></bug><bug fixdate="2014-12-31 05:56:33" id="6147" opendate="2014-12-31 05:30:31"><buginformation><summary>[LUCENE-6147] Make the core Accountables.namedAccountable function public - ASF JIRA</summary><description>Accountables has a number of methods named namedAccountable. The core one of these works by taking a snapshot with an anonymous Accountable. This method is currently private due to concerns over safety. However, I think we should make it public, and document the how safety can be achieved (which is by only using that and the other namedAccountable methods).</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.util.Accountables.java</file></fixedFiles></bug><bug fixdate="2014-12-30 10:19:58" id="6143" opendate="2013-03-13 09:08:03"><buginformation><summary>[LUCENE-6143] Mistake in the comment in source ...surround.parser.QueryParser.jj - ASF JIRA</summary><description>The comment in the source regarding Surround query parser "N is ordered, and W is unordered." is a mistake. Should be the other way around. Appears in in org.apache.lucene.queryparser.surround.parser QueryParser.jj</description></buginformation><fixedFiles><file>lucene.queryparser.src.java.org.apache.lucene.queryparser.surround.parser.QueryParser.java</file></fixedFiles></bug><bug fixdate="2015-01-03 11:25:58" id="6139" opendate="2014-12-26 04:27:00"><buginformation><summary>[LUCENE-6139] TokenGroup.getStart|EndOffset should return matchStart|EndOffset not start|endOffset - ASF JIRA</summary><description>The default highlighter has a TokenGroup class that is passed to Formatter.highlightTerm(). TokenGroup also has getStartOffset() and getEndOffset() methods that ostensibly return the start and end offsets into the original text of the current term. These getters aren't called by Lucene or Solr but they are made available and are useful to me. The problem is that they return the wrong offsets when there are tokens at the same position. I believe this was an oversight of LUCENE-627 in which these getters should have been updated but weren't. The fix is simple: return matchStartOffset and matchEndOffset from these getters, not startOffset and endOffset. I think this oversight would not have occurred if Highlighter didn't have package-access to TokenGroup's fields.</description></buginformation><fixedFiles><file>lucene.highlighter.src.java.org.apache.lucene.search.highlight.Highlighter.java</file><file>lucene.highlighter.src.java.org.apache.lucene.search.highlight.TokenGroup.java</file></fixedFiles></bug><bug fixdate="2014-12-23 03:11:04" id="6131" opendate="2014-12-22 05:06:40"><buginformation><summary>[LUCENE-6131] optimize SortingMergePolicy - ASF JIRA</summary><description>This has a number of performance problems today: suboptimal stored fields merging. This is especially the case with high compression. Today this is 7x-64x times slower than it should be. ram stacking: for any docvalues and norms fields, all instances will be loaded in RAM. for any string docvalues fields, all instances of global ordinals will be built, and none of this released until the whole merge is complete. We can fix these two problems without completely refactoring LeafReader... we won't get a "bulk byte merge", checksum computation will still be suboptimal, and its not a general solution to "merging with filterreaders" but that stuff can be for another day.</description></buginformation><fixedFiles><file>lucene.misc.src.java.org.apache.lucene.index.sorter.SortingMergePolicy.java</file><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.java</file><file>lucene.misc.src.java.org.apache.lucene.search.BlockJoinComparatorSource.java</file><file>lucene.core.src.java.org.apache.lucene.index.SlowCompositeReaderWrapper.java</file><file>lucene.misc.src.test.org.apache.lucene.index.SortingLeafReaderTest.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.IndexSortingTest.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.TestSortingMergePolicy.java</file><file>lucene.misc.src.java.org.apache.lucene.index.sorter.BlockJoinComparatorSource.java</file><file>lucene.misc.src.java.org.apache.lucene.search.EarlyTerminatingSortingCollector.java</file><file>lucene.misc.src.java.org.apache.lucene.index.SortingLeafReader.java</file><file>lucene.misc.src.test.org.apache.lucene.index.SorterTestBase.java</file><file>lucene.misc.src.test.org.apache.lucene.index.TestBlockJoinSorter.java</file><file>lucene.misc.src.test.org.apache.lucene.search.TestEarlyTerminatingCollector.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.SorterTestBase.java</file><file>lucene.misc.src.java.org.apache.lucene.index.Sorter.java</file><file>lucene.misc.src.test.org.apache.lucene.index.IndexSortingTest.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.SortingLeafReaderTest.java</file><file>lucene.misc.src.java.org.apache.lucene.index.MergeReaderWrapper.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.TestEarlyTermination.java</file><file>lucene.misc.src.test.org.apache.lucene.index.TestSortingMergePolicy.java</file><file>lucene.misc.src.java.org.apache.lucene.index.sorter.EarlyTerminatingSortingCollector.java</file><file>lucene.misc.src.test.org.apache.lucene.index.sorter.TestBlockJoinSorter.java</file><file>lucene.misc.src.java.org.apache.lucene.index.SortingMergePolicy.java</file><file>lucene.misc.src.java.org.apache.lucene.index.sorter.Sorter.java</file><file>lucene.misc.src.java.org.apache.lucene.index.sorter.SortingLeafReader.java</file></fixedFiles></bug><bug fixdate="2014-12-22 06:52:36" id="6125" opendate="2014-12-19 12:34:05"><buginformation><summary>[LUCENE-6125] Add more safety checks to MockDirectoryWrapper - ASF JIRA</summary><description>When working on LUCENE-6124, i had to force FSDirectory in my test to find bugs. for performance reasons, no IndexInput/Outputs are doing any checks like ensureOpen when reading and writing data, but MockDirectoryWrapper needs to do this so that tests can find bugs.</description></buginformation><fixedFiles><file>lucene.test-framework.src.java.org.apache.lucene.store.MockIndexOutputWrapper.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.MockIndexInputWrapper.java</file></fixedFiles></bug><bug fixdate="2014-12-19 09:08:00" id="6124" opendate="2014-12-18 05:49:42"><buginformation><summary>[LUCENE-6124] Fix broken close() methods - ASF JIRA</summary><description>Closeable.close() says "If the stream is already closed then invoking this method has no effect.". But a lot of our code does not really respect that. If i add an "extra" close() call in assertingcodec, it finds all kinds of bugs in codec code, for example:    [junit4] Tests with failures (first 10 out of 59):&#13;
   [junit4]   - org.apache.lucene.index.TestCrashCausesCorruptIndex.testCrashCorruptsIndexing&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsOnly&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsAndFreqsAndPositionsAndOffsetsAndPayloads&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsAndFreqs&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsAndFreqsAndPositionsAndOffsets&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testRandom&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsAndFreqsAndPositionsAndPayloads&#13;
   [junit4]   - org.apache.lucene.codecs.asserting.TestAssertingPostingsFormat.testDocsAndFreqsAndPositions&#13;
   [junit4]   - org.apache.lucene.index.TestDirectoryReader.testFilesOpenClose&#13;
   [junit4]   - org.apache.lucene.index.TestIndexWriterDelete.testIndexingThenDeleting</description></buginformation><fixedFiles><file>lucene.codecs.src.java.org.apache.lucene.codecs.blocktreeords.OrdsBlockTreeTermsWriter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.MockDirectoryWrapper.java</file><file>lucene.sandbox.src.java.org.apache.lucene.codecs.idversion.VersionBlockTreeTermsWriter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BasePostingsFormatTestCase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.asserting.AssertingPostingsFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.asserting.AssertingStoredFieldsFormat.java</file><file>lucene.core.src.java.org.apache.lucene.store.NRTCachingDirectory.java</file><file>lucene.codecs.src.java.org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat.java</file><file>lucene.codecs.src.java.org.apache.lucene.codecs.memory.MemoryPostingsFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.asserting.AssertingDocValuesFormat.java</file><file>lucene.core.src.test.org.apache.lucene.store.TestMockDirectoryWrapper.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.BaseDirectoryTestCase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.asserting.AssertingNormsFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseIndexFileFormatTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.asserting.AssertingTermVectorsFormat.java</file></fixedFiles></bug><bug fixdate="2014-12-22 09:50:32" id="6123" opendate="2014-12-18 05:43:47"><buginformation><summary>[LUCENE-6123] BitDocIdSet.Builder.andNot is wrong - ASF JIRA</summary><description>The 2nd part of the body references "sparseSet" instead of "denseSet". We should test it better.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.util.BitDocIdSet.java</file><file>lucene.core.src.test.org.apache.lucene.util.TestBitDocIdSetBuilder.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.BaseBitSetTestCase.java</file></fixedFiles></bug><bug fixdate="2014-12-23 03:48:01" id="6120" opendate="2014-12-17 03:09:31"><buginformation><summary>[LUCENE-6120] how should MockIndexOutputWrapper.close handle exceptions in delegate.close - ASF JIRA</summary><description>Chasing a tricking Elasticsearch test failure, it came down to the delegate.close throwing an exception (ClosedByInterruptException, disturbingly, in this case), causing MockIndexOutputWrapper.close to fail to remove that IO from MDW's map. The question is, what should we do here, when delegate.close throws an exception? Is the delegate in fact closed, even when it throws an exception? Java8's docs on java.io.Closeable say this: As noted in AutoCloseable.close(), cases where the close may fail require careful attention. It is strongly advised to relinquish the underlying resources and to internally mark the Closeable as closed, prior to throwing the IOException. And our OutputStreamIndexOutput is careful about this (flushes, then closes in a try-with-resources). So, I think MDW should be fixed to mark the IO as closed even if delegate.close throws an exception...</description></buginformation><fixedFiles><file>lucene.test-framework.src.java.org.apache.lucene.store.MockIndexOutputWrapper.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.MockIndexInputWrapper.java</file></fixedFiles></bug><bug fixdate="2014-12-17 01:56:20" id="6117" opendate="2014-12-17 01:20:55"><buginformation><summary>[LUCENE-6117] infostream is currently unusable out of box - ASF JIRA</summary><description>testpoints used to only be emitted by assertions (still sketchy), but now are emitted always. I assume this is due to the change to support running tests with assertions disabled. we should try to clean this up, simple stuff like this is now useless: &#13;
indexWriterConfig.setInfoStream(System.out);&#13;
// causes massive flooding like this:&#13;
// TP 0 [Tue Dec 16 20:19:37 EST 2014; Thread-0]: DocumentsWriterPerThread addDocument start&#13;
// TP 0 [Tue Dec 16 20:19:37 EST 2014; Thread-0]: DocumentsWriterPerThread addDocument start&#13;
// TP 0 [Tue Dec 16 20:19:37 EST 2014; Thread-0]: DocumentsWriterPerThread addDocument start&#13;
 I hit this several times today just trying to do benchmarks and debugging.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestStressIndexing2.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriter.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriterPerThread.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.RandomIndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestInfoStream.java</file></fixedFiles></bug><bug fixdate="2015-02-25 10:09:43" id="6105" opendate="2014-12-10 10:05:33"><buginformation><summary>[LUCENE-6105] Don't create root arc cache for tiny FSTs - ASF JIRA</summary><description>The purpose of the root arc cache is to speed up lookups for ASCII terms, but it adds high overhead if the FST is already tiny.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.util.fst.TestFSTs.java</file><file>lucene.core.src.java.org.apache.lucene.util.fst.FST.java</file></fixedFiles></bug><bug fixdate="2014-12-08 07:42:50" id="6097" opendate="2014-12-05 08:20:29"><buginformation><summary>[LUCENE-6097] Make IW.abortMerges and waitForMerges package private - ASF JIRA</summary><description>These are crazy expert methods; must they be public?</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.benchmark.src.test.org.apache.lucene.benchmark.byTask.TestPerfTasksLogic.java</file><file>solr.core.src.java.org.apache.solr.update.DirectUpdateHandler2.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestAddIndexes.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriterReader.java</file><file>lucene.benchmark.src.java.org.apache.lucene.benchmark.byTask.tasks.WaitForMergesTask.java</file><file>lucene.benchmark.src.java.org.apache.lucene.benchmark.byTask.tasks.CloseIndexTask.java</file></fixedFiles></bug><bug fixdate="2014-12-08 09:34:23" id="6094" opendate="2014-12-05 03:52:26"><buginformation><summary>[LUCENE-6094] IW.rollback can take forever when CMS has stalled threads - ASF JIRA</summary><description>CMS hard-stalls incoming threads for denial-of-service protection when merging cannot keep up with whatever is producing new segments. When you call IW.rollback, it asks all merges to abort, and a running merge will periodically check to see if it should abort. However, a stalled merge fails to check, which means rollback can take indefinitely long; I've seen this in Elasticsearch causing shutdown to take &gt; 10 sec.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.ConcurrentMergeScheduler.java</file><file>lucene.core.src.java.org.apache.lucene.index.MergePolicy.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestConcurrentMergeScheduler.java</file></fixedFiles></bug><bug fixdate="2015-02-18 09:24:46" id="6093" opendate="2014-12-04 10:02:27"><buginformation><summary>[LUCENE-6093] BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters in the query - ASF JIRA</summary><description>BlendedInfixSuggester throws NullPointerException if there were discarded trailing characters (e.g. whitespace or special character) in the query. The problem seems to be in the createCoefficient method that fails to check if prefixToken parameter is null. AnalyzingInfixSuggester sets prefixToken to null in the described case and passes it to BlendedInfixSuggester. On the side not even if BlendedInfixSuggester is changed to handle this creates a problem to calculate the weights as prefixToken is null and cannot be used. I would be better to have AnalyzingInfixSuggester to always set prefixToken to lastToken.</description></buginformation><fixedFiles><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.BlendedInfixSuggester.java</file><file>lucene.suggest.src.test.org.apache.lucene.search.suggest.analyzing.BlendedInfixSuggesterTest.java</file></fixedFiles></bug><bug fixdate="2014-12-05 03:28:59" id="6092" opendate="2014-12-03 09:15:51"><buginformation><summary>[LUCENE-6092] Spatial NumberRangePrefixTree wasn't normalizing certain ranges - ASF JIRA</summary><description>NumberRangePrefixTree must normalize/optimize in toRangeShape() to a LeveledValue (not a range) when possible because of assumptions made elsewhere. Plus it should be faster. The case of April to April 1st should yield April 1st, and likewise April 30th to April should yield April 30th. These cases weren't handled leading to a CI test failure (credit to randomized testing).</description></buginformation><fixedFiles><file>lucene.spatial.src.test.org.apache.lucene.spatial.prefix.DateNRStrategyTest.java</file><file>lucene.spatial.src.java.org.apache.lucene.spatial.prefix.tree.NumberRangePrefixTree.java</file><file>lucene.spatial.src.test.org.apache.lucene.spatial.prefix.tree.DateRangePrefixTreeTest.java</file></fixedFiles></bug><bug fixdate="2014-12-02 06:38:25" id="6089" opendate="2014-12-02 05:25:49"><buginformation><summary>[LUCENE-6089] Tune CompressionMode.HIGH_COMPRESSION - ASF JIRA</summary><description>Patch to apply the parameters Adrien proposed on LUCENE-5914. These make this option a lot less costly on CPU and actually compress better too. This only impacts tests.</description></buginformation><fixedFiles><file>lucene.test-framework.src.java.org.apache.lucene.codecs.compressing.dummy.DummyCompressingCodec.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.compressing.CompressingCodec.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.compressing.FastCompressingCodec.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.compressing.FastDecompressionCompressingCodec.java</file><file>lucene.test-framework.src.java.org.apache.lucene.codecs.compressing.HighCompressionCompressingCodec.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.compressing.CompressionMode.java</file></fixedFiles></bug><bug fixdate="2014-12-02 04:45:07" id="6085" opendate="2014-12-01 04:40:58"><buginformation><summary>[LUCENE-6085] Add back SI.attributes (safely) - ASF JIRA</summary><description>We removed this for two reasons: nothing was using it the map is "unsafe" if a codec tried to write to it during in-place dv update. But Adrien has a real use case ( LUCENE-5914), and I think we can just add some safety for the updates case (e.g. if the map is unmodifiable then the trap will not exist, any put() will throw exception). In general, we should have more safety in SI anyway (diagnostics map, too).</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BasePostingsFormatTestCase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseFieldInfoFormatTestCase.java</file><file>lucene.codecs.src.java.org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfoFormat.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseSegmentInfoFormatTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentInfo.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriterPerThread.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDoc.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestSegmentMerger.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestCodecs.java</file><file>lucene.misc.src.java.org.apache.lucene.index.IndexSplitter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseCompoundFormatTestCase.java</file></fixedFiles></bug><bug fixdate="2014-11-26 02:37:20" id="6076" opendate="2014-11-25 11:49:37"><buginformation><summary>[LUCENE-6076] CachingWrapperFilter.getChildResources locks on the wrong object - ASF JIRA</summary><description>CachingWrapperFilter.getChildResources caches on the CachingWrapperFilter instance instead of the wrapped cache.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.search.CachingWrapperFilter.java</file></fixedFiles></bug><bug fixdate="2014-12-01 11:21:36" id="6075" opendate="2014-11-24 12:47:52"><buginformation><summary>[LUCENE-6075] SimpleRateLimiter cast overflow results in Thread.sleep exception - ASF JIRA</summary><description>SimpleRateLimiter.pause() uses an uncheck cast of longs to ints: Thread.sleep((int) (pauseNS/1000000), (int) (pauseNS % 1000000)); Although we check that pauseNS is positive, however if it's large enough the cast to int produces a negative value, causing Thread.sleep to throw an exception. We should protect for it.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.store.RateLimiter.java</file><file>lucene.core.src.test.org.apache.lucene.store.TestRateLimiter.java</file></fixedFiles></bug><bug fixdate="2014-11-23 07:01:04" id="6068" opendate="2014-11-22 03:55:50"><buginformation><summary>[LUCENE-6068] Remove reader.fields() == null checks everywhere - ASF JIRA</summary><description>I don't know how this got this way, but it never returns null. SegmentReader even asserts this. But the api requires consumers to do a bunch of useless null checks. This is a bug.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.MergeState.java</file><file>lucene.core.src.java.org.apache.lucene.index.MultiFields.java</file><file>solr.core.src.java.org.apache.solr.handler.component.TermsComponent.java</file><file>lucene.core.src.java.org.apache.lucene.index.LeafReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.ParallelLeafReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.BufferedUpdatesStream.java</file><file>solr.core.src.test.org.apache.solr.search.TestRTGBase.java</file><file>lucene.core.src.java.org.apache.lucene.index.ExitableDirectoryReader.java</file><file>lucene.misc.src.java.org.apache.lucene.uninverting.DocTermOrds.java</file><file>lucene.core.src.java.org.apache.lucene.search.TermCollectingRewrite.java</file><file>lucene.misc.src.java.org.apache.lucene.misc.HighFreqTerms.java</file><file>lucene.core.src.java.org.apache.lucene.index.TermContext.java</file><file>lucene.core.src.java.org.apache.lucene.search.spans.SpanTermQuery.java</file><file>solr.core.src.java.org.apache.solr.handler.admin.LukeRequestHandler.java</file><file>lucene.core.src.java.org.apache.lucene.index.CheckIndex.java</file><file>lucene.core.src.java.org.apache.lucene.search.MultiTermQueryWrapperFilter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestStressIndexing2.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.AssertingLeafReader.java</file><file>lucene.queries.src.java.org.apache.lucene.queries.function.valuesource.SumTotalTermFreqValueSource.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDocCount.java</file><file>lucene.core.src.java.org.apache.lucene.codecs.FieldsConsumer.java</file><file>solr.core.src.java.org.apache.solr.search.SolrIndexSearcher.java</file><file>lucene.queries.src.java.org.apache.lucene.queries.CommonTermsQuery.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.PerThreadPKLookup.java</file><file>lucene.queries.src.java.org.apache.lucene.queries.TermsFilter.java</file><file>lucene.misc.src.java.org.apache.lucene.index.sorter.SortingLeafReader.java</file></fixedFiles></bug><bug fixdate="2014-11-19 02:39:45" id="6064" opendate="2014-11-18 10:08:20"><buginformation><summary>[LUCENE-6064] throw exception during sort for misconfigured field - ASF JIRA</summary><description>If you sort on field X, and it has no docvalues, today it will silently treat it as "all values missing". This can be very confusing since it just means nothing will happen at all. But there is a distinction between "no docs happen to have a value for this field" and "field isn't configured correctly". The latter should get an exception, telling the user to index docvalues, or wrap the reader with UninvertingReader.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.search.TestSearchAfter.java</file><file>lucene.core.src.test.org.apache.lucene.TestSearch.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestCustomSearcherSort.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocValues.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDocValues.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestFilteredQuery.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestIndexSearcher.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestTopDocsMerge.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestShardSearching.java</file></fixedFiles></bug><bug fixdate="2014-11-19 03:24:42" id="6062" opendate="2014-11-14 08:40:24"><buginformation><summary>[LUCENE-6062] Index corruption from numeric DV updates - ASF JIRA</summary><description>I hit this while working on on LUCENE-6005: when cutting over TestNumericDocValuesUpdates to the new Document2 API, I accidentally enabled additional docValues in the test, and this this: There was 1 failure:&#13;
1) testUpdateSegmentWithNoDocValues(org.apache.lucene.index.TestNumericDocValuesUpdates)&#13;
java.io.FileNotFoundException: _1_Asserting_0.dvm in dir=RAMDirectory@259847e5 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@30981eab&#13;
	at __randomizedtesting.SeedInfo.seed([0:7C88A439A551C47D]:0)&#13;
	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:645)&#13;
	at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:110)&#13;
	at org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer.&lt;init&gt;(Lucene50DocValuesProducer.java:130)&#13;
	at org.apache.lucene.codecs.lucene50.Lucene50DocValuesFormat.fieldsProducer(Lucene50DocValuesFormat.java:182)&#13;
	at org.apache.lucene.codecs.asserting.AssertingDocValuesFormat.fieldsProducer(AssertingDocValuesFormat.java:66)&#13;
	at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsReader.&lt;init&gt;(PerFieldDocValuesFormat.java:267)&#13;
	at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat.fieldsProducer(PerFieldDocValuesFormat.java:357)&#13;
	at org.apache.lucene.index.SegmentDocValues.newDocValuesProducer(SegmentDocValues.java:51)&#13;
	at org.apache.lucene.index.SegmentDocValues.getDocValuesProducer(SegmentDocValues.java:68)&#13;
	at org.apache.lucene.index.SegmentDocValuesProducer.&lt;init&gt;(SegmentDocValuesProducer.java:63)&#13;
	at org.apache.lucene.index.SegmentReader.initDocValuesProducer(SegmentReader.java:167)&#13;
	at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:109)&#13;
	at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:58)&#13;
	at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:50)&#13;
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:556)&#13;
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:50)&#13;
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:63)&#13;
	at org.apache.lucene.index.TestNumericDocValuesUpdates.testUpdateSegmentWithNoDocValues(TestNumericDocValuesUpdates.java:769)&#13;
 A one-line change to the existing test (on trunk) causes this corruption: Index: lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java&#13;
===================================================================&#13;
--- lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java	(revision 1639580)&#13;
+++ lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java	(working copy)&#13;
@@ -750,6 +750,7 @@&#13;
     // second segment with no NDV&#13;
     doc = new Document();&#13;
     doc.add(new StringField("id", "doc1", Store.NO));&#13;
+    doc.add(new NumericDocValuesField("foo", 3));&#13;
     writer.addDocument(doc);&#13;
     doc = new Document();&#13;
     doc.add(new StringField("id", "doc2", Store.NO)); // document that isn't updated&#13;
 For some reason, the base doc values for the 2nd segment is not being written, but clearly should have (to hold field "foo")... I'm not sure why.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.index.TestNumericDocValuesUpdates.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentDocValues.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentReader.java</file><file>lucene.core.src.java.org.apache.lucene.index.SegmentDocValuesProducer.java</file></fixedFiles></bug><bug fixdate="2014-11-13 02:44:53" id="6060" opendate="2014-11-12 10:05:19"><buginformation><summary>[LUCENE-6060] Remove IndexWriter.unLock - ASF JIRA</summary><description>This method used to be necessary, when our locking impls were buggy, but it's a godawful dangerous method: it invites index corruption. I think we should remove it. Apps that for some scary reason really need it can do their own thing...</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.facet.src.java.org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDirectoryReaderReopen.java</file><file>lucene.facet.src.test.org.apache.lucene.facet.taxonomy.TestTaxonomyCombined.java</file><file>solr.core.src.java.org.apache.solr.core.SolrCore.java</file></fixedFiles></bug><bug fixdate="2014-12-05 08:24:50" id="6055" opendate="2014-11-10 08:42:58"><buginformation><summary>[LUCENE-6055] PayloadAttribute.clone() should deep clone its BytesRef - ASF JIRA</summary><description>PayloadAttribute.clone() does a shallow clone, unlike e.g. CharTermAttribute. Attributes should deep clone, otherwise capturing state isn't correct. In addition, both PA's and CTA's .clone() falsely documents that they do shallow cloning on purposes, so need to fix that too.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.util.TestAttributeSource.java</file><file>lucene.core.src.java.org.apache.lucene.util.AttributeImpl.java</file><file>lucene.core.src.java.org.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl.java</file><file>lucene.core.src.java.org.apache.lucene.analysis.tokenattributes.PayloadAttributeImpl.java</file></fixedFiles></bug><bug fixdate="2014-11-07 01:24:37" id="6051" opendate="2014-11-06 02:33:57"><buginformation><summary>[LUCENE-6051] IOUtils methods taking Iterable&lt;? extends Path&gt; try to delete every element of the path - ASF JIRA</summary><description>We have two methods in IOUtils &#13;
 public static void deleteFilesIgnoringExceptions(Iterable&lt;? extends Path&gt; files);&#13;
&#13;
 public static void deleteFilesIfExist(Iterable&lt;? extends Path&gt; files) throws IOException&#13;
 if you call these with a single Path instance it interprets it as Iterable&lt;Path&gt; since Path implements Iternable&lt;Path&gt; and in-turn tries to delete every element of the path. I guess we should fix this before we release. We also need to check if there are other places where we do this... it's nasty...</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.util.IOUtils.java</file><file>lucene.core.src.test.org.apache.lucene.util.TestIOUtils.java</file></fixedFiles></bug><bug fixdate="2014-11-11 09:43:50" id="6050" opendate="2014-11-05 03:50:30"><buginformation><summary>[LUCENE-6050] Add possibility to specify SHOUD or MUST for each context for AnalyzingInfixSuggester.loockup() - ASF JIRA</summary><description>Currently as shown at https://github.com/apache/lucene-solr/blob/lucene_solr_4_9_0/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L362 , we have: &#13;
lookup(CharSequence key, Set&lt;BytesRef&gt; contexts, int num, boolean allTermsRequired, boolean doHighlight)&#13;
 and SHOULD is being applied to all contexts. We need the ability to specify whether it's a SHOULD or a MUST on each individual context. Thanks.</description></buginformation><fixedFiles><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.java</file><file>suggest.src.test.org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggesterTest.java</file><file>lucene.suggest.src.test.org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggesterTest.java</file><file>suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.java</file><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.BlendedInfixSuggester.java</file><file>lucene.suggest.src.test.org.apache.lucene.search.suggest.analyzing.BlendedInfixSuggesterTest.java</file></fixedFiles></bug><bug fixdate="2014-11-08 09:17:32" id="6049" opendate="2014-11-05 09:56:17"><buginformation><summary>[LUCENE-6049] Cryptic exception if all docs in a segment hit non-aborting exceptions before adding their doc values - ASF JIRA</summary><description>I hit this while working on LUCENE-6005: If you add a document with a single field that's both indexed and has doc values, and during inversion it hits a non-aborting exception, and all docs for a given segment had this happen, then you'll hit this confusing exception: java.lang.AssertionError: segment=_0(6.0.0):C2: field="test" has docValues but did not write them&#13;
	at __randomizedtesting.SeedInfo.seed([21BFA52E65A19C81:3A824781C0F77629]:0)&#13;
	at org.apache.lucene.index.DefaultIndexingChain.writeDocValues(DefaultIndexingChain.java:146)&#13;
	at org.apache.lucene.index.DefaultIndexingChain.flush(DefaultIndexingChain.java:93)&#13;
	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:440)&#13;
	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:511)&#13;
	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:622)&#13;
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3016)&#13;
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2992)&#13;
	at org.apache.lucene.index.IndexWriter.shutdown(IndexWriter.java:946)&#13;
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:991)&#13;
	at org.apache.lucene.index.TestDocValuesIndexing.testExcIndexingDocBeforeDocValues(TestDocValuesIndexing.java:927)&#13;
 The good news here is that exception is new from LUCENE-6019 and it prevents this case from causing index corruption, but the bad news is, you shouldn't even get an exception writing the segment in the first place.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.document.SortedSetDocValuesField.java</file><file>lucene.core.src.java.org.apache.lucene.document.Document.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.BaseFieldInfoFormatTestCase.java</file><file>lucene.spatial.src.java.org.apache.lucene.spatial.bbox.BBoxStrategy.java</file><file>lucene.core.src.java.org.apache.lucene.index.DefaultIndexingChain.java</file><file>solr.core.src.java.org.apache.solr.schema.BBoxField.java</file><file>solr.core.src.java.org.apache.solr.schema.FieldType.java</file><file>lucene.core.src.java.org.apache.lucene.index.TermsHash.java</file><file>lucene.core.src.java.org.apache.lucene.document.BinaryDocValuesField.java</file><file>lucene.core.src.test.org.apache.lucene.document.TestFieldType.java</file><file>solr.core.src.java.org.apache.solr.handler.admin.LukeRequestHandler.java</file><file>lucene.core.src.java.org.apache.lucene.index.FieldInfo.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestCodecs.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexableField.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.TestUtil.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestDocValuesIndexing.java</file><file>lucene.core.src.java.org.apache.lucene.index.FieldInfos.java</file><file>lucene.core.src.java.org.apache.lucene.document.FieldType.java</file><file>lucene.core.src.java.org.apache.lucene.index.ReadersAndUpdates.java</file><file>lucene.spatial.src.test.org.apache.lucene.spatial.bbox.TestBBoxStrategy.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestBinaryDocValuesUpdates.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestFieldsReader.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestNumericDocValuesUpdates.java</file><file>lucene.core.src.java.org.apache.lucene.document.NumericDocValuesField.java</file><file>lucene.core.src.java.org.apache.lucene.document.SortedDocValuesField.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexableFieldType.java</file><file>lucene.core.src.java.org.apache.lucene.document.SortedNumericDocValuesField.java</file></fixedFiles></bug><bug fixdate="2014-11-04 10:29:34" id="6046" opendate="2014-11-03 09:28:03"><buginformation><summary>[LUCENE-6046] RegExp.toAutomaton high memory use - ASF JIRA</summary><description>When creating an automaton from an org.apache.lucene.util.automaton.RegExp, it's possible for the automaton to use so much memory it exceeds the maximum array size for java. The following caused an OutOfMemoryError with a 32gb heap: new RegExp("\\[\\[(Datei|File|Bild|Image):[^]]*alt=[^]|}]{50,200}").toAutomaton();&#13;
 When increased to a 60gb heap, the following exception is thrown:   1&gt; java.lang.IllegalArgumentException: requested array size 2147483624 exceeds maximum array in java (2147483623)&#13;
  1&gt;     __randomizedtesting.SeedInfo.seed([7BE81EF678615C32:95C8057A4ABA5B52]:0)&#13;
  1&gt;     org.apache.lucene.util.ArrayUtil.oversize(ArrayUtil.java:168)&#13;
  1&gt;     org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:295)&#13;
  1&gt;     org.apache.lucene.util.automaton.Automaton$Builder.addTransition(Automaton.java:639)&#13;
  1&gt;     org.apache.lucene.util.automaton.Operations.determinize(Operations.java:741)&#13;
  1&gt;     org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:62)&#13;
  1&gt;     org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:51)&#13;
  1&gt;     org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:477)&#13;
  1&gt;     org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:426)</description></buginformation><fixedFiles><file>lucene.sandbox.src.java.org.apache.lucene.search.TermAutomatonQuery.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.AutomatonProvider.java</file><file>solr.core.src.test.org.apache.solr.analysis.TestReversedWildcardFilterFactory.java</file><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.FuzzySuggester.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.ByteRunAutomaton.java</file><file>solr.test-framework.src.java.org.apache.solr.analysis.MockTokenFilterFactory.java</file><file>lucene.core.src.test.org.apache.lucene.analysis.TestMockAnalyzer.java</file><file>lucene.core.src.test.org.apache.lucene.util.fst.TestFSTs.java</file><file>lucene.core.src.java.org.apache.lucene.search.RegexpQuery.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestAutomatonQuery.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestDeterminizeLexicon.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestUTF32ToUTF8.java</file><file>solr.core.src.java.org.apache.solr.parser.SolrQueryParserBase.java</file><file>lucene.queryparser.src.java.org.apache.lucene.queryparser.flexible.standard.builders.RegexpQueryNodeBuilder.java</file><file>lucene.queryparser.src.test.org.apache.lucene.queryparser.util.QueryParserTestBase.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestRegexpQuery.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestLevenshteinAutomata.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.RegExp.java</file><file>lucene.highlighter.src.test.org.apache.lucene.search.vectorhighlight.FieldQueryTest.java</file><file>lucene.queryparser.src.java.org.apache.lucene.queryparser.classic.QueryParserBase.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.automaton.AutomatonTestUtil.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestTermsEnum2.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.MinimizationOperations.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestOperations.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.hunspell.Dictionary.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.Operations.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.CharacterRunAutomaton.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.CompiledAutomaton.java</file><file>lucene.highlighter.src.test.org.apache.lucene.search.highlight.HighlighterTest.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.RegExpTooHardException.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestAutomaton.java</file><file>lucene.core.src.test.org.apache.lucene.search.spans.TestSpanFirstQuery.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestAutomatonQueryUnicode.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestCompiledAutomaton.java</file><file>lucene.core.src.test.org.apache.lucene.util.TestQueryBuilder.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestRegExp.java</file><file>lucene.core.src.test.org.apache.lucene.analysis.TestGraphTokenizers.java</file><file>lucene.suggest.src.java.org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestMinimize.java</file><file>lucene.core.src.test.org.apache.lucene.util.automaton.TestDeterminism.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestRegexpRandom2.java</file><file>lucene.queryparser.src.test.org.apache.lucene.queryparser.flexible.standard.TestQPHelper.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.TooComplexToDeterminizeException.java</file><file>lucene.test-framework.src.java.org.apache.lucene.analysis.MockTokenizer.java</file><file>lucene.core.src.java.org.apache.lucene.util.automaton.RunAutomaton.java</file><file>lucene.highlighter.src.test.org.apache.lucene.search.vectorhighlight.FastVectorHighlighterTest.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestTermsEnum.java</file><file>lucene.core.src.test.org.apache.lucene.codecs.lucene50.TestBlockPostingsFormat3.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.LuceneTestCase.java</file></fixedFiles></bug><bug fixdate="2015-02-07 07:46:41" id="6044" opendate="2014-11-02 04:57:21"><buginformation><summary>[LUCENE-6044] Add backcompat for TokenFilters with posInc=false before 4.4 - ASF JIRA</summary><description>In Lucene 4.4, a number of token filters supporting the enablePositionIncrements=false setting were changed to default to true. However, with Lucene 5.0, the setting was removed altogether. We should have backcompat for this setting, as well as work when used with a TokenFilterFactory and match version &lt; 4.4.</description></buginformation><fixedFiles><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.miscellaneous.KeepWordFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.miscellaneous.LengthFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestTrimFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.core.TestTypeTokenFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestKeepFilterFactory.java</file><file>lucene.analysis.kuromoji.src.java.org.apache.lucene.analysis.ja.JapanesePartOfSpeechStopFilterFactory.java</file><file>lucene.analysis.kuromoji.src.test.org.apache.lucene.analysis.ja.TestJapanesePartOfSpeechStopFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.core.TypeTokenFilterFactory.java</file><file>lucene.analysis.kuromoji.src.java.org.apache.lucene.analysis.ja.Lucene43JapanesePartOfSpeechStopFilter.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.core.TestStopFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.miscellaneous.TestLengthFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.core.StopFilterFactory.java</file></fixedFiles></bug><bug fixdate="2014-11-01 08:01:02" id="6043" opendate="2014-11-01 07:33:17"><buginformation><summary>[LUCENE-6043] Add backcompat support for UAX29URLEmailTokenizer before 4.7 - ASF JIRA</summary><description>In LUCENE-5999 backcompat support was added for StandardTokenizer with unicode 6.1, but UAX29URLEmailTokenizer was overlooked.</description></buginformation><fixedFiles><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.StandardTokenizerFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.en.TestEnglishAnalyzer.java</file><file>lucene.analysis.uima.src.test.org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.EdgeNGramFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.hi.HindiAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.fi.TestFinnishAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.en.EnglishAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.nl.DutchAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.cz.CzechAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.cjk.TestCJKAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.standard.TestUAX29URLEmailTokenizer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.tr.TestTurkishAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.sv.SwedishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.br.TestBrazilianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.fa.PersianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.fr.FrenchAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.lv.TestLatvianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.it.TestItalianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.hy.ArmenianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.fa.TestPersianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.bg.TestBulgarianAnalyzer.java</file><file>lucene.analysis.smartcn.src.java.org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.eu.BasqueAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ngram.EdgeNGramTokenFilterTest.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.UAX29URLEmailTokenizerFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ckb.TestSoraniAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ca.CatalanAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.hu.HungarianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.NGramFilterFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.UAX29URLEmailAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.cjk.CJKAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ru.RussianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.de.GermanAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.nl.TestDutchAnalyzer.java</file><file>lucene.backward-codecs.src.java.org.apache.lucene.codecs.Placeholder.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ga.IrishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.hi.TestHindiAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.std40.UAX29URLEmailTokenizerImpl40.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.fi.FinnishAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.gl.GalicianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ngram.NGramTokenFilterTest.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ro.TestRomanianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.es.SpanishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.el.GreekAnalyzerTest.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.it.ItalianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.no.NorwegianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.standard.TestStandardAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ar.ArabicAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.pt.PortugueseAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.th.ThaiAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ga.TestIrishAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.br.BrazilianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.NGramTokenFilter.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.no.TestNorwegianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.id.TestIndonesianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.compound.DictionaryCompoundWordTokenFilterFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.hy.TestArmenianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.da.DanishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.sv.TestSwedishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.eu.TestBasqueAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.da.TestDanishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.cz.TestCzechAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ar.TestArabicAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.es.TestSpanishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.gl.TestGalicianAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ngram.NGramTokenizerTest.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.de.TestGermanAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.standard.TestUAX29URLEmailAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.core.TestRandomChains.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.ClassicTokenizerImpl.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.hu.TestHungarianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.bg.BulgarianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.lv.LatvianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.standard.StandardAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ru.TestRussianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.tr.TurkishAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.th.TestThaiAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.NGramTokenizer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.fr.TestFrenchAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ckb.SoraniAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.NGramTokenizerFactory.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.el.GreekAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.id.IndonesianAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ngram.EdgeNGramTokenizerFactory.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.pt.TestPortugueseAnalyzer.java</file><file>lucene.analysis.common.src.test.org.apache.lucene.analysis.ca.TestCatalanAnalyzer.java</file><file>lucene.analysis.common.src.java.org.apache.lucene.analysis.ro.RomanianAnalyzer.java</file></fixedFiles></bug><bug fixdate="2014-12-01 03:27:48" id="6042" opendate="2014-10-31 11:21:34"><buginformation><summary>[LUCENE-6042] CustomScoreQuery Explain differs from the actual score when topLevelBoost is used. - ASF JIRA</summary><description>CustomScoreQuery.java, doExplain has the following line: &#13;
res.addDetail(new Explanation(getBoost(), "queryBoost"));&#13;
 This multiplies the custom score query by just the boost of the current query, and not by &#13;
queryWeight=topLevelBoost*getBoost();&#13;
 which is the value that's actually used during scoring. This leads to drastically different scores in the debug info, relative to the actual score, when the query is a subquery of another one, like a BooleanQuery clause, with a non-1 boost.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.search.TestSimpleExplanations.java</file><file>lucene.test-framework.src.java.org.apache.lucene.search.BaseExplanationTestCase.java</file><file>lucene.core.src.test.org.apache.lucene.search.payloads.TestPayloadExplanations.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestExplanations.java</file><file>lucene.core.src.test.org.apache.lucene.search.spans.TestSpanExplanations.java</file><file>lucene.queries.src.test.org.apache.lucene.queries.TestCustomScoreExplanations.java</file><file>lucene.core.src.test.org.apache.lucene.search.TestComplexExplanations.java</file><file>lucene.queries.src.java.org.apache.lucene.queries.CustomScoreQuery.java</file></fixedFiles></bug><bug fixdate="2014-11-05 09:05:26" id="6038" opendate="2014-10-31 08:19:05"><buginformation><summary>[LUCENE-6038] FieldValueFilter regression - ASF JIRA</summary><description>The decoupling of FixedBitSet from a DocIdSet ( LUCENE-5441) introduced a regression in FieldValueFilter, which checks if the bits for documents with a field is an instance of a DocIdSet. Yet FixedBitSet does not extend DocIdSet anymore.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.search.TestFieldValueFilter.java</file><file>lucene.core.src.java.org.apache.lucene.search.FieldValueFilter.java</file></fixedFiles></bug><bug fixdate="2014-10-30 05:05:32" id="6036" opendate="2014-10-30 04:52:25"><buginformation><summary>[LUCENE-6036] TestIndexWriterOnJRECrash suffers from JDK-8047340 - needs workarround - ASF JIRA</summary><description>Similar to issues uncovered in SOLR-6387, TestIndexWriterOnJRECrash does some forking which can hit JDK-8047340 on the turkish locale...    [junit4] Suite: org.apache.lucene.index.TestIndexWriterOnJRECrash&#13;
   [junit4]   2&gt; NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.&#13;
   [junit4]   2&gt; NOTE: reproduce with: ant test  -Dtestcase=TestIndexWriterOnJRECrash -Dtests.method=testNRTThreads -Dtests.seed=B2D360EA192CA242 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=tr -Dtests.timezone=Africa/Monrovia -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1&#13;
   [junit4] ERROR   0.04s J0 | TestIndexWriterOnJRECrash.testNRTThreads &lt;&lt;&lt;&#13;
   [junit4]    &gt; Throwable #1: java.lang.Error: posix_spawn is not a supported process launch mechanism on this platform.&#13;
   [junit4]    &gt; 	at __randomizedtesting.SeedInfo.seed([B2D360EA192CA242:290A74F158D7B429]:0)&#13;
   [junit4]    &gt; 	at java.lang.UNIXProcess$1.run(UNIXProcess.java:111)&#13;
   [junit4]    &gt; 	at java.lang.UNIXProcess$1.run(UNIXProcess.java:93)&#13;
   [junit4]    &gt; 	at java.security.AccessController.doPrivileged(Native Method)&#13;
   [junit4]    &gt; 	at java.lang.UNIXProcess.&lt;clinit&gt;(UNIXProcess.java:91)&#13;
   [junit4]    &gt; 	at java.lang.ProcessImpl.start(ProcessImpl.java:130)&#13;
   [junit4]    &gt; 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)&#13;
   [junit4]    &gt; 	at org.apache.lucene.index.TestIndexWriterOnJRECrash.forkTest(TestIndexWriterOnJRECrash.java:113)&#13;
   [junit4]    &gt; 	at org.apache.lucene.index.TestIndexWriterOnJRECrash.testNRTThreads(TestIndexWriterOnJRECrash.java:59)&#13;
   [junit4]    &gt; 	at java.lang.Thread.run(Thread.java:745)&#13;
 https://bugs.openjdk.java.net/browse/JDK-8047340</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriterOnJRECrash.java</file></fixedFiles></bug><bug fixdate="2014-10-28 06:41:56" id="6027" opendate="2014-10-27 04:49:35"><buginformation><summary>[LUCENE-6027] Fix visibility issues in field comparators - ASF JIRA</summary><description>These comparators cannot currently be instantiated because although the classes are public, the constructors are package-private (except Strings for some reason). The visibility was correct in 4.10, e.g. can be used to plug in your own sources of raw values like SortedSet/Numeric SortField do.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.search.FieldComparator.java</file></fixedFiles></bug><bug fixdate="2014-10-27 04:33:32" id="6026" opendate="2014-10-27 03:18:54"><buginformation><summary>[LUCENE-6026] Give AbstractPagedMutable 'Accountable' rather than its own ramBytesUsed() api - ASF JIRA</summary><description>This should just implement Accountable rather than re-specifying ramBytesUsed.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.util.packed.AbstractPagedMutable.java</file></fixedFiles></bug><bug fixdate="2014-12-18 09:27:48" id="6019" opendate="2014-10-22 08:43:18"><buginformation><summary>[LUCENE-6019] IndexWriter allows to add same field with different docvlaues type - ASF JIRA</summary><description>IndexWriter checks if the DV types are consitent in multiple places but if due to some problems in Elasticsearch users where able to add the same field with different DV types causing merges to fail. Yet I was able to reduce this to a lucene testcase but I was puzzled since it always failed. Yet, I had to run it without assertions and that cause the bug to happen. I can add field foo with BINARY and SORTED_SET causing a merge to fail. Here is a gist https://gist.github.com/s1monw/8707f924b76ba40ee5f3 / https://github.com/elasticsearch/elasticsearch/issues/8009 While this is certainly a problem in Elasticsearch Lucene also allows to corrupt an index due to user error which I think should be prevented. NOTE: this only fails if you run without assertions which I think lucene should do in CI once in a while too.</description></buginformation><fixedFiles><file>lucene.core.src.test.org.apache.lucene.index.TestDocValuesIndexing.java</file><file>lucene.test-framework.src.java.org.apache.lucene.store.MockDirectoryWrapper.java</file><file>lucene.core.src.test.org.apache.lucene.index.TestIndexWriterExceptions.java</file><file>lucene.core.src.java.org.apache.lucene.analysis.TokenStream.java</file><file>lucene.core.src.java.org.apache.lucene.index.FieldInfos.java</file><file>lucene.core.src.java.org.apache.lucene.index.TermVectorsConsumer.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.TestRuleAssertionsRequired.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.RunListenerPrintReproduceInfo.java</file><file>lucene.core.src.java.org.apache.lucene.index.IndexWriter.java</file><file>lucene.core.src.test.org.apache.lucene.TestAssertions.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriterPerThread.java</file><file>lucene.test-framework.src.java.org.apache.lucene.index.RandomIndexWriter.java</file><file>lucene.test-framework.src.java.org.apache.lucene.util.LuceneTestCase.java</file><file>lucene.core.src.java.org.apache.lucene.index.FreqProxTermsWriterPerField.java</file><file>lucene.core.src.java.org.apache.lucene.index.TermVectorsConsumerPerField.java</file><file>lucene.core.src.java.org.apache.lucene.index.DocumentsWriterStallControl.java</file></fixedFiles></bug><bug fixdate="2014-10-21 09:11:03" id="6015" opendate="2014-10-20 09:13:31"><buginformation><summary>[LUCENE-6015] Revisit DocIdSetBuilder's heuristic to switch to FixedBitSet - ASF JIRA</summary><description>DocIdSetBuilder starts with a SparseFixedBitSet and then upgrades to a FixedBitSet when the cardinality grows larger than maxDoc &gt;&gt;&gt; 14. However Robert improved SparseFixedBitSet performance quite significantly in LUCENE-6003 so we should see if it makes sense to update this heuristic.</description></buginformation><fixedFiles><file>lucene.core.src.java.org.apache.lucene.util.DocIdSetBuilder.java</file></fixedFiles></bug></bugrepository>